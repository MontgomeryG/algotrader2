{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ELEMENTARY TRADING BOT\n",
    "\n",
    "The below is a start at using 3 machine learning models to create long/short signals in instances where the stock trades in a range. We will look to employ the algorithm during instances defined with the following characteristics:\n",
    "- Large-cap stocks (they tend to offer ranges intraday that \"scalpers\" take advantage of. I'm willing to bet an algo can learn how to scalp like those traders or even better.)\n",
    "- During times of low volatility. Low Volatility is essentially what we mean when we say, \"the stock is trading in a range\"; when the price action of a stock can be considered \"range-bound\".  This is typically in the middle of the day when there is less volume in the markets. We can visualize the average volume given time of day later on in this project. \n",
    "\n",
    "We will start with the above, keeing it simple.\n",
    "\n",
    "\n",
    "Our next step is to choose the variables that we will feed the algorithm. Remember that these variables will be based on a stock's Open Low High and Close (OHLC), and Volume, for now..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TO--DO\n",
    "Now we determine the stocks of which we'd like to pull the OHLC data.\n",
    "Let's go with all of the stocks in the NQ and the ES. This should serve as a nice basis for how the type of stocks that we want to deploy our algorithm onto move; ***mega-cap stocks that offer range-bound trading during periods of low volatility.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting a list of the stocks in the NQ and ES onto a dataframe. \n",
    "\n",
    "Let's start with the NQ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import hvplot.pandas\n",
    "from pathlib import Path\n",
    "from finta import TA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Read csv, take a look\n",
    "# nq_df = pd.read_csv((\"../tradingbotmg/resources/nq_stocks.csv\"))\n",
    "# nq_df.head()\n",
    "\n",
    "# COMMENTING OUT NOW, We will do this later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Do the same for the snp 500 symbols\n",
    "# snp_df = pd.read_csv((\"../tradingbotmg/resources/snp_stocks.csv\"))\n",
    "# snp_df.head()\n",
    "\n",
    "# COMMENTING OUT NOW, We will do this later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save the symbols from each df\n",
    "# nq_symbols_df = nq_df[\"Symbol\"]\n",
    "# snp_symbols_df = snp_df[\"Symbol\"]\n",
    "\n",
    "# # Put dfs together\n",
    "# snp_nq_symbols_df = pd.concat([nq_symbols_df, snp_symbols_df], axis=0)\n",
    "\n",
    "# # Drop duplicates\n",
    "# snp_nq_symbols_df.drop_duplicates(inplace=True)\n",
    "\n",
    "# # View\n",
    "# snp_nq_symbols_df\n",
    "\n",
    "# COMMENTING OUT NOW, We will do this later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Convert symbols to a list so that we can feed it into our Alpaca API.\n",
    "# # Alpaca is where we will access the historical OHLC data needed.\n",
    "\n",
    "# nq_snp_symbols_list = snp_nq_symbols_df.tolist()\n",
    "\n",
    "# len(nq_snp_symbols_list)\n",
    "\n",
    "# COMMENTING OUT NOW, We will do this later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # THE FOLLOWING IS WHEN WE CONTINUE WITH MANY STOCKS...\n",
    "# # THE FOLLOWING NEEDS TO BE ITERATED\n",
    "\n",
    "# # Rearrange df\n",
    "# # Separate ticker data\n",
    "\n",
    "# # WE'LL HAVE TO iterate the below for all stocks in both the SPY and NQ\n",
    "\n",
    "# AAPL = aapl_msft_df[aapl_msft_df['symbol']=='AAPL'].drop('symbol', axis=1)\n",
    "# MSFT = aapl_msft_df[aapl_msft_df['symbol']=='MSFT'].drop('symbol', axis=1)\n",
    "\n",
    "# AAPL.sort_index(inplace=True)\n",
    "# MSFT.sort_index(inplace=True)\n",
    "\n",
    "# # Concatenate the ticker DataFrames\n",
    "# am_df = pd.concat([AAPL, MSFT],axis=1, keys=['AAPL','MSFT'])\n",
    "\n",
    "\n",
    "# am_df\n",
    "\n",
    "\n",
    "# # FEATURE CREATION WITH TWO stocks... this is for LATER USE\n",
    "\n",
    "# # Generate returns from close column with pct_change\n",
    "\n",
    "# # WILL NEED TO INTERATE\n",
    "# am_df[(\"AAPL\",\"actual_returns\")] = am_df[(\"AAPL\",\"close\")].pct_change()\n",
    "# am_df[(\"MSFT\",\"actual_returns\")] = am_df[(\"MSFT\",\"close\")].pct_change()\n",
    "\n",
    "# # Drop NaN\n",
    "# am_df = am_df.dropna()\n",
    "\n",
    "# # Simple Moving Averages\n",
    "# am_df[(\"AAPL\",\"AAPL_SMA5\")] = TA.SMA(am_df[\"AAPL\"],5)\n",
    "# am_df[(\"AAPL\",\"AAPL_SMA10\")] = TA.SMA(am_df[\"AAPL\"],10)\n",
    "\n",
    "# am_df[(\"MSFT\",\"MSFT_SMA5\")] = TA.SMA(am_df[\"MSFT\"],5)\n",
    "# am_df[(\"MSFT\",\"MSFT_SMA10\")] = TA.SMA(am_df[\"MSFT\"],10)\n",
    "\n",
    "# # Exponential Moving Averages\n",
    "# am_df[(\"AAPL\",\"AAPL_EMA3\")] = TA.EMA(am_df[\"AAPL\"],3)\n",
    "\n",
    "# am_df[(\"MSFT\",\"MSFT_EMA3\")] = TA.EMA(am_df[\"MSFT\"],3)\n",
    "\n",
    "# # NEEDS TO BE ITERATED!!\n",
    "\n",
    "\n",
    "# # Create a df with all the indicators for both AAPL and MSFT\n",
    "# aapl_indicator_df = am_df[\"AAPL\"].drop([\"actual_returns\"], axis=1)\n",
    "# msft_indicator_df = am_df[\"MSFT\"].drop([\"actual_returns\"], axis=1)\n",
    "\n",
    "# # Save the signal column as our 'y'\n",
    "# y=am_df[\"AAPL\"][\"signal\"]\n",
    "\n",
    "# display(aapl_indicator_df.head())\n",
    "# display(aapl_indicator_df.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay. Now let's set up our alpaca API to get the OHLC data.\n",
    "\n",
    "\n",
    "### Alpaca API for Historical OHLC Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports for Alpaca API, .env files, requests\n",
    "import os\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "import alpaca_trade_api as tradeapi\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load .env enviroment variables\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpaca Key type: <class 'str'>\n",
      "Alpaca Secret Key type: <class 'str'>\n"
     ]
    }
   ],
   "source": [
    "# Set Alpaca API key and secret\n",
    "alpaca_api_key = os.getenv(\"ALPACA_API_KEY\")\n",
    "alpaca_secret_key = os.getenv(\"ALPACA_SECRET_KEY\")\n",
    "\n",
    "# Verify that Alpaca key and secret were correctly loaded\n",
    "print(f\"Alpaca Key type: {type(alpaca_api_key)}\")\n",
    "print(f\"Alpaca Secret Key type: {type(alpaca_secret_key)}\")\n",
    "\n",
    "# Create the Alpaca API object\n",
    "alpaca = tradeapi.REST(\n",
    "    alpaca_api_key,\n",
    "    alpaca_secret_key,\n",
    "    api_version=\"v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set start and end dates and specify isoformat\n",
    "# We will start off with on week in the middle of November of this year, 2023.\n",
    "\n",
    "start_date = pd.Timestamp(\"2023-01-01\", tz=\"America/New_York\").isoformat()\n",
    "end_date = pd.Timestamp(\"2024-01-17\", tz=\"America/New_York\").isoformat()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the tickers\n",
    "# Test with one for now.\n",
    "# Later, this is where we will feed in our list of nq/es stocks we created above\n",
    "tickers = [\"AAPL\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set timeframe for Alpaca API\n",
    "timeframe = \"1Min\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get historical OHLC data for AAPL\n",
    "aapl_df = alpaca.get_bars(\n",
    "    tickers,\n",
    "    timeframe,\n",
    "    start = start_date,\n",
    "    end = end_date\n",
    ").df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>close</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>trade_count</th>\n",
       "      <th>open</th>\n",
       "      <th>volume</th>\n",
       "      <th>vwap</th>\n",
       "      <th>symbol</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timestamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2023-01-03 09:00:00+00:00</th>\n",
       "      <td>131.00</td>\n",
       "      <td>131.00</td>\n",
       "      <td>130.28</td>\n",
       "      <td>208</td>\n",
       "      <td>130.28</td>\n",
       "      <td>8174</td>\n",
       "      <td>130.854120</td>\n",
       "      <td>AAPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-03 09:01:00+00:00</th>\n",
       "      <td>131.10</td>\n",
       "      <td>131.17</td>\n",
       "      <td>130.87</td>\n",
       "      <td>157</td>\n",
       "      <td>130.87</td>\n",
       "      <td>8820</td>\n",
       "      <td>130.955243</td>\n",
       "      <td>AAPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-03 09:02:00+00:00</th>\n",
       "      <td>131.17</td>\n",
       "      <td>131.24</td>\n",
       "      <td>131.17</td>\n",
       "      <td>53</td>\n",
       "      <td>131.18</td>\n",
       "      <td>2112</td>\n",
       "      <td>131.208769</td>\n",
       "      <td>AAPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-03 09:03:00+00:00</th>\n",
       "      <td>131.28</td>\n",
       "      <td>131.29</td>\n",
       "      <td>131.19</td>\n",
       "      <td>90</td>\n",
       "      <td>131.19</td>\n",
       "      <td>3888</td>\n",
       "      <td>131.220790</td>\n",
       "      <td>AAPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-03 09:04:00+00:00</th>\n",
       "      <td>131.46</td>\n",
       "      <td>131.46</td>\n",
       "      <td>131.28</td>\n",
       "      <td>88</td>\n",
       "      <td>131.28</td>\n",
       "      <td>5984</td>\n",
       "      <td>131.327899</td>\n",
       "      <td>AAPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-01-17 00:55:00+00:00</th>\n",
       "      <td>183.00</td>\n",
       "      <td>183.03</td>\n",
       "      <td>183.00</td>\n",
       "      <td>45</td>\n",
       "      <td>183.03</td>\n",
       "      <td>1224</td>\n",
       "      <td>183.014261</td>\n",
       "      <td>AAPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-01-17 00:56:00+00:00</th>\n",
       "      <td>183.00</td>\n",
       "      <td>183.01</td>\n",
       "      <td>183.00</td>\n",
       "      <td>69</td>\n",
       "      <td>183.01</td>\n",
       "      <td>4641</td>\n",
       "      <td>183.001462</td>\n",
       "      <td>AAPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-01-17 00:57:00+00:00</th>\n",
       "      <td>183.01</td>\n",
       "      <td>183.04</td>\n",
       "      <td>183.01</td>\n",
       "      <td>21</td>\n",
       "      <td>183.04</td>\n",
       "      <td>958</td>\n",
       "      <td>183.030350</td>\n",
       "      <td>AAPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-01-17 00:58:00+00:00</th>\n",
       "      <td>183.05</td>\n",
       "      <td>183.05</td>\n",
       "      <td>183.04</td>\n",
       "      <td>21</td>\n",
       "      <td>183.04</td>\n",
       "      <td>671</td>\n",
       "      <td>183.043748</td>\n",
       "      <td>AAPL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-01-17 00:59:00+00:00</th>\n",
       "      <td>183.04</td>\n",
       "      <td>183.04</td>\n",
       "      <td>183.04</td>\n",
       "      <td>36</td>\n",
       "      <td>183.04</td>\n",
       "      <td>1574</td>\n",
       "      <td>183.043148</td>\n",
       "      <td>AAPL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>194711 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            close    high     low  trade_count    open  \\\n",
       "timestamp                                                                \n",
       "2023-01-03 09:00:00+00:00  131.00  131.00  130.28          208  130.28   \n",
       "2023-01-03 09:01:00+00:00  131.10  131.17  130.87          157  130.87   \n",
       "2023-01-03 09:02:00+00:00  131.17  131.24  131.17           53  131.18   \n",
       "2023-01-03 09:03:00+00:00  131.28  131.29  131.19           90  131.19   \n",
       "2023-01-03 09:04:00+00:00  131.46  131.46  131.28           88  131.28   \n",
       "...                           ...     ...     ...          ...     ...   \n",
       "2024-01-17 00:55:00+00:00  183.00  183.03  183.00           45  183.03   \n",
       "2024-01-17 00:56:00+00:00  183.00  183.01  183.00           69  183.01   \n",
       "2024-01-17 00:57:00+00:00  183.01  183.04  183.01           21  183.04   \n",
       "2024-01-17 00:58:00+00:00  183.05  183.05  183.04           21  183.04   \n",
       "2024-01-17 00:59:00+00:00  183.04  183.04  183.04           36  183.04   \n",
       "\n",
       "                           volume        vwap symbol  \n",
       "timestamp                                             \n",
       "2023-01-03 09:00:00+00:00    8174  130.854120   AAPL  \n",
       "2023-01-03 09:01:00+00:00    8820  130.955243   AAPL  \n",
       "2023-01-03 09:02:00+00:00    2112  131.208769   AAPL  \n",
       "2023-01-03 09:03:00+00:00    3888  131.220790   AAPL  \n",
       "2023-01-03 09:04:00+00:00    5984  131.327899   AAPL  \n",
       "...                           ...         ...    ...  \n",
       "2024-01-17 00:55:00+00:00    1224  183.014261   AAPL  \n",
       "2024-01-17 00:56:00+00:00    4641  183.001462   AAPL  \n",
       "2024-01-17 00:57:00+00:00     958  183.030350   AAPL  \n",
       "2024-01-17 00:58:00+00:00     671  183.043748   AAPL  \n",
       "2024-01-17 00:59:00+00:00    1574  183.043148   AAPL  \n",
       "\n",
       "[194711 rows x 8 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display sample data\n",
    "aapl_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FOR NOW, we will not include pre-market/after-hours action, but WE WILL in the future.\n",
    "\n",
    "Now we clean up the AAPL and MSFT dfs we created; put them together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate ticker data\n",
    "AAPL = aapl_df[aapl_df['symbol']=='AAPL'].drop('symbol', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>close</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>trade_count</th>\n",
       "      <th>open</th>\n",
       "      <th>volume</th>\n",
       "      <th>vwap</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timestamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2023-01-03 09:00:00+00:00</th>\n",
       "      <td>131.00</td>\n",
       "      <td>131.00</td>\n",
       "      <td>130.28</td>\n",
       "      <td>208</td>\n",
       "      <td>130.28</td>\n",
       "      <td>8174</td>\n",
       "      <td>130.854120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-03 09:01:00+00:00</th>\n",
       "      <td>131.10</td>\n",
       "      <td>131.17</td>\n",
       "      <td>130.87</td>\n",
       "      <td>157</td>\n",
       "      <td>130.87</td>\n",
       "      <td>8820</td>\n",
       "      <td>130.955243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-03 09:02:00+00:00</th>\n",
       "      <td>131.17</td>\n",
       "      <td>131.24</td>\n",
       "      <td>131.17</td>\n",
       "      <td>53</td>\n",
       "      <td>131.18</td>\n",
       "      <td>2112</td>\n",
       "      <td>131.208769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-03 09:03:00+00:00</th>\n",
       "      <td>131.28</td>\n",
       "      <td>131.29</td>\n",
       "      <td>131.19</td>\n",
       "      <td>90</td>\n",
       "      <td>131.19</td>\n",
       "      <td>3888</td>\n",
       "      <td>131.220790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-03 09:04:00+00:00</th>\n",
       "      <td>131.46</td>\n",
       "      <td>131.46</td>\n",
       "      <td>131.28</td>\n",
       "      <td>88</td>\n",
       "      <td>131.28</td>\n",
       "      <td>5984</td>\n",
       "      <td>131.327899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-01-17 00:55:00+00:00</th>\n",
       "      <td>183.00</td>\n",
       "      <td>183.03</td>\n",
       "      <td>183.00</td>\n",
       "      <td>45</td>\n",
       "      <td>183.03</td>\n",
       "      <td>1224</td>\n",
       "      <td>183.014261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-01-17 00:56:00+00:00</th>\n",
       "      <td>183.00</td>\n",
       "      <td>183.01</td>\n",
       "      <td>183.00</td>\n",
       "      <td>69</td>\n",
       "      <td>183.01</td>\n",
       "      <td>4641</td>\n",
       "      <td>183.001462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-01-17 00:57:00+00:00</th>\n",
       "      <td>183.01</td>\n",
       "      <td>183.04</td>\n",
       "      <td>183.01</td>\n",
       "      <td>21</td>\n",
       "      <td>183.04</td>\n",
       "      <td>958</td>\n",
       "      <td>183.030350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-01-17 00:58:00+00:00</th>\n",
       "      <td>183.05</td>\n",
       "      <td>183.05</td>\n",
       "      <td>183.04</td>\n",
       "      <td>21</td>\n",
       "      <td>183.04</td>\n",
       "      <td>671</td>\n",
       "      <td>183.043748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-01-17 00:59:00+00:00</th>\n",
       "      <td>183.04</td>\n",
       "      <td>183.04</td>\n",
       "      <td>183.04</td>\n",
       "      <td>36</td>\n",
       "      <td>183.04</td>\n",
       "      <td>1574</td>\n",
       "      <td>183.043148</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>194711 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            close    high     low  trade_count    open  \\\n",
       "timestamp                                                                \n",
       "2023-01-03 09:00:00+00:00  131.00  131.00  130.28          208  130.28   \n",
       "2023-01-03 09:01:00+00:00  131.10  131.17  130.87          157  130.87   \n",
       "2023-01-03 09:02:00+00:00  131.17  131.24  131.17           53  131.18   \n",
       "2023-01-03 09:03:00+00:00  131.28  131.29  131.19           90  131.19   \n",
       "2023-01-03 09:04:00+00:00  131.46  131.46  131.28           88  131.28   \n",
       "...                           ...     ...     ...          ...     ...   \n",
       "2024-01-17 00:55:00+00:00  183.00  183.03  183.00           45  183.03   \n",
       "2024-01-17 00:56:00+00:00  183.00  183.01  183.00           69  183.01   \n",
       "2024-01-17 00:57:00+00:00  183.01  183.04  183.01           21  183.04   \n",
       "2024-01-17 00:58:00+00:00  183.05  183.05  183.04           21  183.04   \n",
       "2024-01-17 00:59:00+00:00  183.04  183.04  183.04           36  183.04   \n",
       "\n",
       "                           volume        vwap  \n",
       "timestamp                                      \n",
       "2023-01-03 09:00:00+00:00    8174  130.854120  \n",
       "2023-01-03 09:01:00+00:00    8820  130.955243  \n",
       "2023-01-03 09:02:00+00:00    2112  131.208769  \n",
       "2023-01-03 09:03:00+00:00    3888  131.220790  \n",
       "2023-01-03 09:04:00+00:00    5984  131.327899  \n",
       "...                           ...         ...  \n",
       "2024-01-17 00:55:00+00:00    1224  183.014261  \n",
       "2024-01-17 00:56:00+00:00    4641  183.001462  \n",
       "2024-01-17 00:57:00+00:00     958  183.030350  \n",
       "2024-01-17 00:58:00+00:00     671  183.043748  \n",
       "2024-01-17 00:59:00+00:00    1574  183.043148  \n",
       "\n",
       "[194711 rows x 7 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AAPL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the indexes\n",
    "\n",
    "AAPL.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Concatenate the ticker DataFrames\n",
    "# AAPL = pd.concat([AAPL, MSFT],axis=1, keys=['AAPL','MSFT'])\n",
    "\n",
    "# # Preview DataFrame\n",
    "# AAPL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice. Now we have the OHLC data here in our workspace, and we can move onto the next step of the process; data cleaning.\n",
    "\n",
    "### Data Cleaning\n",
    "\n",
    "Drop NA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>close</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>trade_count</th>\n",
       "      <th>open</th>\n",
       "      <th>volume</th>\n",
       "      <th>vwap</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timestamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2023-01-03 09:00:00+00:00</th>\n",
       "      <td>131.00</td>\n",
       "      <td>131.00</td>\n",
       "      <td>130.28</td>\n",
       "      <td>208</td>\n",
       "      <td>130.28</td>\n",
       "      <td>8174</td>\n",
       "      <td>130.854120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-03 09:01:00+00:00</th>\n",
       "      <td>131.10</td>\n",
       "      <td>131.17</td>\n",
       "      <td>130.87</td>\n",
       "      <td>157</td>\n",
       "      <td>130.87</td>\n",
       "      <td>8820</td>\n",
       "      <td>130.955243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-03 09:02:00+00:00</th>\n",
       "      <td>131.17</td>\n",
       "      <td>131.24</td>\n",
       "      <td>131.17</td>\n",
       "      <td>53</td>\n",
       "      <td>131.18</td>\n",
       "      <td>2112</td>\n",
       "      <td>131.208769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-03 09:03:00+00:00</th>\n",
       "      <td>131.28</td>\n",
       "      <td>131.29</td>\n",
       "      <td>131.19</td>\n",
       "      <td>90</td>\n",
       "      <td>131.19</td>\n",
       "      <td>3888</td>\n",
       "      <td>131.220790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-03 09:04:00+00:00</th>\n",
       "      <td>131.46</td>\n",
       "      <td>131.46</td>\n",
       "      <td>131.28</td>\n",
       "      <td>88</td>\n",
       "      <td>131.28</td>\n",
       "      <td>5984</td>\n",
       "      <td>131.327899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-01-17 00:55:00+00:00</th>\n",
       "      <td>183.00</td>\n",
       "      <td>183.03</td>\n",
       "      <td>183.00</td>\n",
       "      <td>45</td>\n",
       "      <td>183.03</td>\n",
       "      <td>1224</td>\n",
       "      <td>183.014261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-01-17 00:56:00+00:00</th>\n",
       "      <td>183.00</td>\n",
       "      <td>183.01</td>\n",
       "      <td>183.00</td>\n",
       "      <td>69</td>\n",
       "      <td>183.01</td>\n",
       "      <td>4641</td>\n",
       "      <td>183.001462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-01-17 00:57:00+00:00</th>\n",
       "      <td>183.01</td>\n",
       "      <td>183.04</td>\n",
       "      <td>183.01</td>\n",
       "      <td>21</td>\n",
       "      <td>183.04</td>\n",
       "      <td>958</td>\n",
       "      <td>183.030350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-01-17 00:58:00+00:00</th>\n",
       "      <td>183.05</td>\n",
       "      <td>183.05</td>\n",
       "      <td>183.04</td>\n",
       "      <td>21</td>\n",
       "      <td>183.04</td>\n",
       "      <td>671</td>\n",
       "      <td>183.043748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-01-17 00:59:00+00:00</th>\n",
       "      <td>183.04</td>\n",
       "      <td>183.04</td>\n",
       "      <td>183.04</td>\n",
       "      <td>36</td>\n",
       "      <td>183.04</td>\n",
       "      <td>1574</td>\n",
       "      <td>183.043148</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>194711 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            close    high     low  trade_count    open  \\\n",
       "timestamp                                                                \n",
       "2023-01-03 09:00:00+00:00  131.00  131.00  130.28          208  130.28   \n",
       "2023-01-03 09:01:00+00:00  131.10  131.17  130.87          157  130.87   \n",
       "2023-01-03 09:02:00+00:00  131.17  131.24  131.17           53  131.18   \n",
       "2023-01-03 09:03:00+00:00  131.28  131.29  131.19           90  131.19   \n",
       "2023-01-03 09:04:00+00:00  131.46  131.46  131.28           88  131.28   \n",
       "...                           ...     ...     ...          ...     ...   \n",
       "2024-01-17 00:55:00+00:00  183.00  183.03  183.00           45  183.03   \n",
       "2024-01-17 00:56:00+00:00  183.00  183.01  183.00           69  183.01   \n",
       "2024-01-17 00:57:00+00:00  183.01  183.04  183.01           21  183.04   \n",
       "2024-01-17 00:58:00+00:00  183.05  183.05  183.04           21  183.04   \n",
       "2024-01-17 00:59:00+00:00  183.04  183.04  183.04           36  183.04   \n",
       "\n",
       "                           volume        vwap  \n",
       "timestamp                                      \n",
       "2023-01-03 09:00:00+00:00    8174  130.854120  \n",
       "2023-01-03 09:01:00+00:00    8820  130.955243  \n",
       "2023-01-03 09:02:00+00:00    2112  131.208769  \n",
       "2023-01-03 09:03:00+00:00    3888  131.220790  \n",
       "2023-01-03 09:04:00+00:00    5984  131.327899  \n",
       "...                           ...         ...  \n",
       "2024-01-17 00:55:00+00:00    1224  183.014261  \n",
       "2024-01-17 00:56:00+00:00    4641  183.001462  \n",
       "2024-01-17 00:57:00+00:00     958  183.030350  \n",
       "2024-01-17 00:58:00+00:00     671  183.043748  \n",
       "2024-01-17 00:59:00+00:00    1574  183.043148  \n",
       "\n",
       "[194711 rows x 7 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop NA\n",
    "AAPL = AAPL.dropna()\n",
    "\n",
    "AAPL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For 1 minute: Dropping the NaN just took out about 600 rows of data. Did it take out the whole column given the date? That is not good, since if we have the data for one stock at a given minute but not the other, we don't want to throw out that info for the one stock. \n",
    "\n",
    "3166 to 2405 rows after dropping NaN.\n",
    "\n",
    "We will come back to this. Remember, we don't want little things like this from holding us back from implementing this. Just make a note of these little issues and crush them during the next iteration. \n",
    "\n",
    "UPDATE 1: With 3 minutes, we start with 1200 rows, then have 1014 after dropping NaN. \n",
    "\n",
    "Seems to get better when we zoom out... We will most probably just source better data, however. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now save this cleaned dataframe to a new .csv\n",
    "AAPL.to_csv('../algotrader2/Resources/aapl_OHLCV_1min_df.csv', index=\"True\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our next step is to generate the features from the data set that we will train our model with. \n",
    "\n",
    "[Feature Creation->](/Users/montygash/Desktop/ETAlgo/tradingbot/nn_feature_creation.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the indicators and what we are trying to predict (actual returns). \n",
    "\n",
    "We will set the indicators to our 'X' and the actual return as our 'y'.\n",
    "\n",
    "In this case, we have two different stocks that we are trying to predict.\n",
    "- AAPL data will be used to predict AAPL actual returns.\n",
    "- MSFT data will be used to predict MSFT actual returns. \n",
    "- Is this how we want the algorithm to be? To only train itself on a particular stock? \n",
    "    - No... I want it to be trained on MULTIPLE stocks. So it is trained on both MSFT, AAPL, and even TSLA, AMD, NVDA... and then I deploy it on any stock that I want to deploy it onto. \n",
    "    - This is a major issue that I need to resolve. \n",
    "\n",
    "\n",
    "Let's just begin by implementing the neural network algorithm on only AAPL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This begs another question: Do we only need to shift it back one? Since we are using 1 minute candles? Maybe we should be shifting it back more than one one minute bar?... We will revisit this later.\n",
    "\n",
    "\n",
    "We are trying to predict y, which is whether or not we should buy or sell the stock at that given time.\n",
    "\n",
    "I was confused and tried to implement by making actual returns be the y, but we actually want to generate signals, where the bot chooses whether to short or long the stock based on the predicted returns during the next 1 minute candle (we will probably revisit the 1-minute candle idea. I'm not sure that we want the bot to send out that many signals. Maybe we train it with different OHLC time-frames)\n",
    "\n",
    "So the first step in making the bot generate signals is to define when the signal is 1 (buy), and -1(sell).\n",
    "\n",
    "We want the bot to sell when the expected returns is negative, and buy when positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Signal Column\n",
    "AAPL[(\"AAPL\",\"signal\")] = 0.0\n",
    "\n",
    "AAPL[\"AAPL\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the signal column as our 'y'\n",
    "y=AAPL[\"AAPL\"][\"signal\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Split the Data into Training set and Testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import DateOffset\n",
    "from pandas.tseries.offsets import DateOffset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set start of training period\n",
    "\n",
    "training_begin = X.index.min()\n",
    "\n",
    "# Display training begin date\n",
    "\n",
    "print(training_begin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select ending period for the training data. Let's use 3 days, since we are pulling one week of data to start with.\n",
    "training_end = X.index.min() + DateOffset(days=3)\n",
    "\n",
    "# Show training end date\n",
    "print(training_end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So our \"starter\" model will start on the 13th of November, use November 13th to 16th to train itself, and then it will predict the price action of the last day in the set, November 17th. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the X_train and y_train DataFrames\n",
    "X_train = X.loc[training_begin:training_end]\n",
    "y_train = y.loc[training_begin:training_end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the X_test and y_test DataFrames\n",
    "X_test = X.loc[training_end:]\n",
    "y_test = y.loc[training_end:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our train and test sets, we shall standardize the data.\n",
    "\n",
    "### Standardize the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import scaler library\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a StandardScaler instance\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Apply the scaler model to fit the X-train data\n",
    "X_scaler = scaler.fit(X_train)\n",
    "\n",
    "# Transform the X_train and X_test DataFrames using the X_scaler\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can implement our machine learning on the scaled data. Let's first use Support Vector Machines, to keep it simple, before we implement the neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the SVM model\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the classifier model\n",
    "svm_model = svm.SVC()\n",
    "\n",
    "# Fit the model to the data using X_train_scaled and y_train\n",
    "svm_model = svm_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Use the trained model to predict the trading signals for the training data\n",
    "training_signal_predictions = svm_model.predict(X_train_scaled)\n",
    "\n",
    "# Display the sample predictions\n",
    "training_signal_predictions[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model using a classification report\n",
    "training_report = classification_report(y_train, training_signal_predictions)\n",
    "\n",
    "# Display report\n",
    "print(training_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backtesting the Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the trained model to predict the trading signals for the testing data.\n",
    "testing_signal_predictions = svm_model.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model's ability to predict the trading signal for the testing data\n",
    "svm_testing_report = classification_report(y_test, testing_signal_predictions)\n",
    "\n",
    "# Display the report\n",
    "print(svm_testing_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visually Compare Actual vs Predicted Returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a predictions DataFrame\n",
    "predictions_df = pd.DataFrame(index=X_test.index)\n",
    "\n",
    "predictions_df[\"predicted_signal\"] = testing_signal_predictions\n",
    "\n",
    "predictions_df[\"actual_returns\"] = AAPL['AAPL'][\"actual_returns\"]\n",
    "\n",
    "predictions_df[\"trading_algorithm_returns\"] = (\n",
    "    predictions_df[\"actual_returns\"] * predictions_df[\"predicted_signal\"]\n",
    ")\n",
    "\n",
    "# Review the DataFrame\n",
    "predictions_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and plot the cumulative returns for the `actual_returns` and the `trading_algorithm_returns`\n",
    "(1 + predictions_df[[\"actual_returns\", \"trading_algorithm_returns\"]]).cumprod().plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like our algorithm is missing the point. I wonder if we should go back into the code because we missed something. \n",
    "\n",
    "We will do that, but first let's use a different ML model for our predictions and see how different the results are.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import LogisticRegression from sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the LogisticRegression model\n",
    "logistic_regression_model = LogisticRegression()\n",
    "\n",
    "# Fit the LogisticRegression model\n",
    "logistic_regression_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Use the trained LogisticRegression model to predict the trading signals for the training data\n",
    "lr_training_signal_predictions = logistic_regression_model.predict(X_train_scaled)\n",
    "\n",
    "# Display the predictions\n",
    "lr_training_signal_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a classification report using the training data and the logistic regression model's predications\n",
    "lr_training_report = classification_report(y_train, lr_training_signal_predictions)\n",
    "\n",
    "# Review the classification report\n",
    "print(lr_training_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "About the same as the SVM model. \n",
    "\n",
    "Time to backtest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the trained model to predict the trading signals for the testing data.\n",
    "lr_testing_signal_predictions = logistic_regression_model.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a classification report using the testing data and the logistic regression model's predictions\n",
    "lr_testing_report = classification_report(y_test, lr_testing_signal_predictions)\n",
    "\n",
    "# Review the testing classification report\n",
    "print(lr_testing_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the classification report for the SVM model using the test data\n",
    "print(\"SVM Classification Report\")\n",
    "print(svm_testing_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I guess the SVM model couldn't predict when to buy well... Or there is something wrong with our implementation. How can our recall and f1-score be that low for predicting when to buy, and so high when predicting when to sell? We will revisit this.\n",
    "\n",
    "Now let's try a basic neural networks model with the same data. \n",
    "\n",
    "### Neural Networks Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Activation, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_predictors = len(X.columns)\n",
    "\n",
    "# We have a binary outcome, so one output\n",
    "num_classes = 1\n",
    "\n",
    "num_predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add dense layer(s)\n",
    "nn_model.add(Dense(10, input_dim=num_predictors, activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop-out layer(s)\n",
    "# nn_model.add(Dropout(.2,input_shape=(10,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add dense layer, add Regularization\n",
    "#model.add(Dense(5, activation='relu', kernel_regularized=l2(0.01), bias_regularized=l2(0.01)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add output layer\n",
    "# Number of outputs equals number of classes\n",
    "#nn_model.add(Dense(num_classes))\n",
    "nn_model.add(Dense(num_classes, activation=\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "nn_model.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer=\"adam\",\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize model\n",
    "nn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit model\n",
    "num_epochs = 50\n",
    "\n",
    "nn_model.fit(X_train_scaled, y_train,\n",
    "          epochs=num_epochs,\n",
    "          batch_size=100,\n",
    "          validation_split=0.2,     # This 'validation_split' is telling the neural network to keep 20% of the data to validate its score on the training set... this is to help AVOID OVERFITTING. \n",
    "          shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show model loss and accuracy\n",
    "\n",
    "# Evaluate the model loss and accuracy metrics using the evaluate method and the test data\n",
    "model_loss, model_accuracy = nn_model.evaluate(X_test_scaled, y_test, verbose=2)\n",
    "\n",
    "# Display the evaluation results\n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict values using testing data\n",
    "nn_test_predictions = nn_model.predict(X_test_scaled)\n",
    "nn_train_predictions = nn_model.predict(X_train_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model using Classification Report\n",
    "\n",
    "#train_report = classification_report(y_test, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save model history for further manipulation\n",
    "# model_history = model.history.model_history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Now we can plot the accuracy for training and validation\n",
    "\n",
    "# training_results = pd.DataFrame(index=range(1, num_epochs+1))\n",
    "# training_results['Training'] = model_history['categorical_accuracy']\n",
    "# training_results['Validation'] = model_history['val_categorical_accuracy']\n",
    "# training_results.plot(title = 'Training and Validation Performance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First iteration of the neural network model implemented! It is not performing very well, however...\n",
    "\n",
    "We will come back and fine-tune it, but for right now let's get our model's predictions linked up to Alpaca's trade API so we can execute trades based on the model's decisions.\n",
    "\n",
    "### Linking to Alpaca Trade API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set signal variable\n",
    "signal = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create buy signal, num shares and ticker\n",
    "if signal == 1:\n",
    "    orderSide = \"buy\"\n",
    "else:\n",
    "    orderSide = \"sell\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the ticket symbol and the number of shares to buy\n",
    "ticker = \"AAPL\"\n",
    "number_of_shares = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Make API call\n",
    "# prices = alpaca.get_bars(ticker, \"1Min\").df\n",
    "\n",
    "# # Reorganize the DataFrame\n",
    "# prices = pd.concat([prices], axis=1, keys=[\"AAPL\"])\n",
    "\n",
    "# # Get final closing price\n",
    "# limit_amount = prices[\"AAPL\"][\"close\"][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Submit order\n",
    "# alpaca.submit_order(\n",
    "#     symbol=\"AAPL\", \n",
    "#     qty=number_of_shares, \n",
    "#     side=orderSide, \n",
    "#     time_in_force=\"gtc\", \n",
    "#     type=\"limit\", \n",
    "#     limit_price=limit_amount\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "deploy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice! Now I want to take only the necessary portion of this page onto a new .ipynb. We will make it nice and clean. We will only include the data prep portion and the neural networks model. We will keep our comments more breif to improve the readability of the overall idea--the 'gist' of all of the moving parts--from somewhat of a bird's eye view. We will divide the code up strategically, so that we can dive into each moving part and tune it as needed. Our model should become more and more sofisticated and accurate over future iterations until our goal of consistient profitability is met.\n",
    "\n",
    "[Neural Networks Trading Bot->](neural_networks_trading_bot.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
