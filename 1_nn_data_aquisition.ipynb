{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ELEMENTARY TRADING BOT\n",
    "\n",
    "The below is a start at using 3 machine learning models to create long/short signals in instances where the stock trades in a range. We will look to employ the algorithm during instances defined with the following characteristics:\n",
    "- Large-cap stocks (they tend to offer ranges intraday that \"scalpers\" take advantage of. I'm willing to bet an algo can learn how to scalp like those traders or even better.)\n",
    "- During times of low volatility. Low Volatility is essentially what we mean when we say, \"the stock is trading in a range\"; when the price action of a stock can be considered \"range-bound\".  This is typically in the middle of the day when there is less volume in the markets. We can visualize the average volume given time of day later on in this project. \n",
    "\n",
    "We will start with the above, keeing it simple.\n",
    "\n",
    "\n",
    "Our next step is to choose the variables that we will feed the algorithm. Remember that these variables will be based on a stock's Open Low High and Close (OHLC), and Volume, for now..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TO--DO\n",
    "Now we determine the stocks of which we'd like to pull the OHLC data.\n",
    "Let's go with all of the stocks in the NQ and the ES. This should serve as a nice basis for how the type of stocks that we want to deploy our algorithm onto move; ***mega-cap stocks that offer range-bound trading during periods of low volatility.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting a list of the stocks in the NQ and ES onto a dataframe. \n",
    "\n",
    "Let's start with the NQ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import hvplot.pandas\n",
    "from pathlib import Path\n",
    "from finta import TA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Read csv, take a look\n",
    "# nq_df = pd.read_csv((\"../algotrader2/resources/nq_stocks.csv\"))\n",
    "# nq_df.head()\n",
    "\n",
    "# COMMENTING OUT NOW, We will do this later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Do the same for the snp 500 symbols\n",
    "# snp_df = pd.read_csv((\"../algotrader2/resources/snp_stocks.csv\"))\n",
    "# snp_df.head()\n",
    "\n",
    "# COMMENTING OUT NOW, We will do this later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save the symbols from each df\n",
    "# nq_symbols_df = nq_df[\"Symbol\"]\n",
    "# snp_symbols_df = snp_df[\"Symbol\"]\n",
    "\n",
    "# # Put dfs together\n",
    "# snp_nq_symbols_df = pd.concat([nq_symbols_df, snp_symbols_df], axis=0)\n",
    "\n",
    "# # Drop duplicates\n",
    "# snp_nq_symbols_df.drop_duplicates(inplace=True)\n",
    "\n",
    "# # View\n",
    "# snp_nq_symbols_df\n",
    "\n",
    "# COMMENTING OUT NOW, We will do this later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Convert symbols to a list so that we can feed it into our Alpaca API.\n",
    "# # Alpaca is where we will access the historical OHLC data needed.\n",
    "\n",
    "# nq_snp_symbols_list = snp_nq_symbols_df.tolist()\n",
    "\n",
    "# len(nq_snp_symbols_list)\n",
    "\n",
    "# COMMENTING OUT NOW, We will do this later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # THE FOLLOWING IS WHEN WE CONTINUE WITH MANY STOCKS...\n",
    "# # THE FOLLOWING NEEDS TO BE ITERATED\n",
    "\n",
    "# # Rearrange df\n",
    "# # Separate ticker data\n",
    "\n",
    "# # WE'LL HAVE TO iterate the below for all stocks in both the SPY and NQ\n",
    "\n",
    "# AAPL = aapl_msft_df[aapl_msft_df['symbol']=='AAPL'].drop('symbol', axis=1)\n",
    "# MSFT = aapl_msft_df[aapl_msft_df['symbol']=='MSFT'].drop('symbol', axis=1)\n",
    "\n",
    "# AAPL.sort_index(inplace=True)\n",
    "# MSFT.sort_index(inplace=True)\n",
    "\n",
    "# # Concatenate the ticker DataFrames\n",
    "# am_df = pd.concat([AAPL, MSFT],axis=1, keys=['AAPL','MSFT'])\n",
    "\n",
    "\n",
    "# am_df\n",
    "\n",
    "\n",
    "# # FEATURE CREATION WITH TWO stocks... this is for LATER USE\n",
    "\n",
    "# # Generate returns from close column with pct_change\n",
    "\n",
    "# # WILL NEED TO INTERATE\n",
    "# am_df[(\"AAPL\",\"actual_returns\")] = am_df[(\"AAPL\",\"close\")].pct_change()\n",
    "# am_df[(\"MSFT\",\"actual_returns\")] = am_df[(\"MSFT\",\"close\")].pct_change()\n",
    "\n",
    "# # Drop NaN\n",
    "# am_df = am_df.dropna()\n",
    "\n",
    "# # Simple Moving Averages\n",
    "# am_df[(\"AAPL\",\"AAPL_SMA5\")] = TA.SMA(am_df[\"AAPL\"],5)\n",
    "# am_df[(\"AAPL\",\"AAPL_SMA10\")] = TA.SMA(am_df[\"AAPL\"],10)\n",
    "\n",
    "# am_df[(\"MSFT\",\"MSFT_SMA5\")] = TA.SMA(am_df[\"MSFT\"],5)\n",
    "# am_df[(\"MSFT\",\"MSFT_SMA10\")] = TA.SMA(am_df[\"MSFT\"],10)\n",
    "\n",
    "# # Exponential Moving Averages\n",
    "# am_df[(\"AAPL\",\"AAPL_EMA3\")] = TA.EMA(am_df[\"AAPL\"],3)\n",
    "\n",
    "# am_df[(\"MSFT\",\"MSFT_EMA3\")] = TA.EMA(am_df[\"MSFT\"],3)\n",
    "\n",
    "# # NEEDS TO BE ITERATED!!\n",
    "\n",
    "\n",
    "# # Create a df with all the indicators for both AAPL and MSFT\n",
    "# aapl_indicator_df = am_df[\"AAPL\"].drop([\"actual_returns\"], axis=1)\n",
    "# msft_indicator_df = am_df[\"MSFT\"].drop([\"actual_returns\"], axis=1)\n",
    "\n",
    "# # Save the signal column as our 'y'\n",
    "# y=am_df[\"AAPL\"][\"signal\"]\n",
    "\n",
    "# display(aapl_indicator_df.head())\n",
    "# display(aapl_indicator_df.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay. Now let's set up our alpaca API to get the OHLC data.\n",
    "\n",
    "\n",
    "### Alpaca API for Historical OHLC Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports for Alpaca API, .env files, requests\n",
    "import os\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "import alpaca_trade_api as tradeapi\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load .env enviroment variables\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Alpaca API key and secret\n",
    "alpaca_api_key = os.getenv(\"ALPACA_API_KEY\")\n",
    "alpaca_secret_key = os.getenv(\"ALPACA_SECRET_KEY\")\n",
    "\n",
    "# Verify that Alpaca key and secret were correctly loaded\n",
    "print(f\"Alpaca Key type: {type(alpaca_api_key)}\")\n",
    "print(f\"Alpaca Secret Key type: {type(alpaca_secret_key)}\")\n",
    "\n",
    "# Create the Alpaca API object\n",
    "alpaca = tradeapi.REST(\n",
    "    alpaca_api_key,\n",
    "    alpaca_secret_key,\n",
    "    api_version=\"v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set start and end dates and specify isoformat\n",
    "# We will start off with on week in the middle of November of this year, 2023.\n",
    "\n",
    "start_date = pd.Timestamp(\"2023-01-01\", tz=\"America/New_York\").isoformat()\n",
    "end_date = pd.Timestamp(\"2024-01-17\", tz=\"America/New_York\").isoformat()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the tickers\n",
    "# Test with one for now.\n",
    "# Later, this is where we will feed in our list of nq/es stocks we created above\n",
    "tickers = [\"AAPL\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set timeframe for Alpaca API\n",
    "timeframe = \"1Min\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get historical OHLC data for AAPL\n",
    "aapl_df = alpaca.get_bars(\n",
    "    tickers,\n",
    "    timeframe,\n",
    "    start = start_date,\n",
    "    end = end_date\n",
    ").df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample data\n",
    "aapl_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FOR NOW, we will not include pre-market/after-hours action, but WE WILL in the future.\n",
    "\n",
    "Now we clean up the AAPL and MSFT dfs we created; put them together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate ticker data\n",
    "AAPL = aapl_df[aapl_df['symbol']=='AAPL'].drop('symbol', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AAPL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the indexes\n",
    "\n",
    "AAPL.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Concatenate the ticker DataFrames\n",
    "# AAPL = pd.concat([AAPL, MSFT],axis=1, keys=['AAPL','MSFT'])\n",
    "\n",
    "# # Preview DataFrame\n",
    "# AAPL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice. Now we have the OHLC data here in our workspace, and we can move onto the next step of the process; data cleaning.\n",
    "\n",
    "### Data Cleaning\n",
    "\n",
    "Drop NA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop NA\n",
    "AAPL = AAPL.dropna()\n",
    "\n",
    "AAPL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For 1 minute: Dropping the NaN just took out about 600 rows of data. Did it take out the whole column given the date? That is not good, since if we have the data for one stock at a given minute but not the other, we don't want to throw out that info for the one stock. \n",
    "\n",
    "3166 to 2405 rows after dropping NaN.\n",
    "\n",
    "We will come back to this. Remember, we don't want little things like this from holding us back from implementing this. Just make a note of these little issues and crush them during the next iteration. \n",
    "\n",
    "UPDATE 1: With 3 minutes, we start with 1200 rows, then have 1014 after dropping NaN. \n",
    "\n",
    "Seems to get better when we zoom out... We will most probably just source better data, however. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now save this cleaned dataframe to a new .csv\n",
    "AAPL.to_csv('../algotrader2/Resources/aapl_OHLCV_1min_df.csv', index=\"True\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our next step is to generate the features from the data set that we will train our model with. \n",
    "\n",
    "[Feature Creation->](/Users/montygash/Desktop/ETAlgo/tradingbot/nn_feature_creation.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the indicators and what we are trying to predict (actual returns). \n",
    "\n",
    "We will set the indicators to our 'X' and the actual return as our 'y'.\n",
    "\n",
    "In this case, we have two different stocks that we are trying to predict.\n",
    "- AAPL data will be used to predict AAPL actual returns.\n",
    "- MSFT data will be used to predict MSFT actual returns. \n",
    "- Is this how we want the algorithm to be? To only train itself on a particular stock? \n",
    "    - No... I want it to be trained on MULTIPLE stocks. So it is trained on both MSFT, AAPL, and even TSLA, AMD, NVDA... and then I deploy it on any stock that I want to deploy it onto. \n",
    "    - This is a major issue that I need to resolve. \n",
    "\n",
    "\n",
    "Let's just begin by implementing the neural network algorithm on only AAPL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This begs another question: Do we only need to shift it back one? Since we are using 1 minute candles? Maybe we should be shifting it back more than one one minute bar?... We will revisit this later.\n",
    "\n",
    "\n",
    "We are trying to predict y, which is whether or not we should buy or sell the stock at that given time.\n",
    "\n",
    "I was confused and tried to implement by making actual returns be the y, but we actually want to generate signals, where the bot chooses whether to short or long the stock based on the predicted returns during the next 1 minute candle (we will probably revisit the 1-minute candle idea. I'm not sure that we want the bot to send out that many signals. Maybe we train it with different OHLC time-frames)\n",
    "\n",
    "So the first step in making the bot generate signals is to define when the signal is 1 (buy), and -1(sell).\n",
    "\n",
    "We want the bot to sell when the expected returns is negative, and buy when positive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "deploy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice! Now I want to take only the necessary portion of this page onto a new .ipynb. We will make it nice and clean. We will only include the data prep portion and the neural networks model. We will keep our comments more breif to improve the readability of the overall idea--the 'gist' of all of the moving parts--from somewhat of a bird's eye view. We will divide the code up strategically, so that we can dive into each moving part and tune it as needed. Our model should become more and more sofisticated and accurate over future iterations until our goal of consistient profitability is met."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
