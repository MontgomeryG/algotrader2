{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import hvplot.pandas\n",
    "from pathlib import Path\n",
    "from finta import TA\n",
    "from pandas.tseries.offsets import DateOffset\n",
    "import os\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "import alpaca_trade_api as tradeapi\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Activation, Dropout, LSTM\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>close</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>trade_count</th>\n",
       "      <th>open</th>\n",
       "      <th>volume</th>\n",
       "      <th>vwap</th>\n",
       "      <th>pct_returns</th>\n",
       "      <th>SMA5</th>\n",
       "      <th>...</th>\n",
       "      <th>CMO</th>\n",
       "      <th>FISH</th>\n",
       "      <th>SQZMI</th>\n",
       "      <th>VPT</th>\n",
       "      <th>FVE</th>\n",
       "      <th>VFI</th>\n",
       "      <th>MSD</th>\n",
       "      <th>STC</th>\n",
       "      <th>signal</th>\n",
       "      <th>new_signal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-01-09 21:30:00+00:00</td>\n",
       "      <td>130.0817</td>\n",
       "      <td>130.15</td>\n",
       "      <td>130.0400</td>\n",
       "      <td>314</td>\n",
       "      <td>130.09</td>\n",
       "      <td>18713</td>\n",
       "      <td>130.096350</td>\n",
       "      <td>0.000167</td>\n",
       "      <td>130.19634</td>\n",
       "      <td>...</td>\n",
       "      <td>-53.399594</td>\n",
       "      <td>-2.609553</td>\n",
       "      <td>False</td>\n",
       "      <td>-6.975113e+07</td>\n",
       "      <td>-22.378567</td>\n",
       "      <td>9.776626</td>\n",
       "      <td>1.066249</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-01-09 21:45:00+00:00</td>\n",
       "      <td>130.1000</td>\n",
       "      <td>130.14</td>\n",
       "      <td>130.1000</td>\n",
       "      <td>182</td>\n",
       "      <td>130.11</td>\n",
       "      <td>4711</td>\n",
       "      <td>130.121440</td>\n",
       "      <td>0.000141</td>\n",
       "      <td>130.11634</td>\n",
       "      <td>...</td>\n",
       "      <td>-51.671029</td>\n",
       "      <td>-2.459362</td>\n",
       "      <td>False</td>\n",
       "      <td>-6.975348e+07</td>\n",
       "      <td>-23.502623</td>\n",
       "      <td>10.152024</td>\n",
       "      <td>1.046721</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-01-09 22:00:00+00:00</td>\n",
       "      <td>130.0600</td>\n",
       "      <td>130.12</td>\n",
       "      <td>130.0300</td>\n",
       "      <td>128</td>\n",
       "      <td>130.12</td>\n",
       "      <td>6111</td>\n",
       "      <td>130.076383</td>\n",
       "      <td>-0.000307</td>\n",
       "      <td>130.09034</td>\n",
       "      <td>...</td>\n",
       "      <td>-52.958418</td>\n",
       "      <td>-2.603288</td>\n",
       "      <td>False</td>\n",
       "      <td>-6.976163e+07</td>\n",
       "      <td>-24.565646</td>\n",
       "      <td>9.791596</td>\n",
       "      <td>1.011924</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-01-09 22:15:00+00:00</td>\n",
       "      <td>130.0700</td>\n",
       "      <td>130.09</td>\n",
       "      <td>130.0315</td>\n",
       "      <td>210</td>\n",
       "      <td>130.06</td>\n",
       "      <td>95004</td>\n",
       "      <td>130.135637</td>\n",
       "      <td>0.000077</td>\n",
       "      <td>130.07434</td>\n",
       "      <td>...</td>\n",
       "      <td>-51.834922</td>\n",
       "      <td>-2.888556</td>\n",
       "      <td>False</td>\n",
       "      <td>-6.972915e+07</td>\n",
       "      <td>-25.441689</td>\n",
       "      <td>10.514562</td>\n",
       "      <td>0.966759</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-01-09 22:30:00+00:00</td>\n",
       "      <td>130.0600</td>\n",
       "      <td>130.08</td>\n",
       "      <td>130.0300</td>\n",
       "      <td>148</td>\n",
       "      <td>130.04</td>\n",
       "      <td>4724</td>\n",
       "      <td>130.045182</td>\n",
       "      <td>-0.000077</td>\n",
       "      <td>130.07434</td>\n",
       "      <td>...</td>\n",
       "      <td>-52.224826</td>\n",
       "      <td>-3.240848</td>\n",
       "      <td>False</td>\n",
       "      <td>-6.972537e+07</td>\n",
       "      <td>-26.410294</td>\n",
       "      <td>11.958663</td>\n",
       "      <td>0.907764</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   timestamp     close    high       low  trade_count    open  \\\n",
       "0  2023-01-09 21:30:00+00:00  130.0817  130.15  130.0400          314  130.09   \n",
       "1  2023-01-09 21:45:00+00:00  130.1000  130.14  130.1000          182  130.11   \n",
       "2  2023-01-09 22:00:00+00:00  130.0600  130.12  130.0300          128  130.12   \n",
       "3  2023-01-09 22:15:00+00:00  130.0700  130.09  130.0315          210  130.06   \n",
       "4  2023-01-09 22:30:00+00:00  130.0600  130.08  130.0300          148  130.04   \n",
       "\n",
       "   volume        vwap  pct_returns       SMA5  ...        CMO      FISH  \\\n",
       "0   18713  130.096350     0.000167  130.19634  ... -53.399594 -2.609553   \n",
       "1    4711  130.121440     0.000141  130.11634  ... -51.671029 -2.459362   \n",
       "2    6111  130.076383    -0.000307  130.09034  ... -52.958418 -2.603288   \n",
       "3   95004  130.135637     0.000077  130.07434  ... -51.834922 -2.888556   \n",
       "4    4724  130.045182    -0.000077  130.07434  ... -52.224826 -3.240848   \n",
       "\n",
       "   SQZMI           VPT        FVE        VFI       MSD  STC  signal  \\\n",
       "0  False -6.975113e+07 -22.378567   9.776626  1.066249  0.0       0   \n",
       "1  False -6.975348e+07 -23.502623  10.152024  1.046721  0.0       0   \n",
       "2  False -6.976163e+07 -24.565646   9.791596  1.011924  0.0       0   \n",
       "3  False -6.972915e+07 -25.441689  10.514562  0.966759  0.0       0   \n",
       "4  False -6.972537e+07 -26.410294  11.958663  0.907764  0.0       0   \n",
       "\n",
       "   new_signal  \n",
       "0           1  \n",
       "1           1  \n",
       "2           1  \n",
       "3           1  \n",
       "4           1  \n",
       "\n",
       "[5 rows x 100 columns]"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../algotrader2/resources/aapl_15min_pivot_point_indicator_df.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our timestamp column as a datetime index, then save it as our index\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "df.set_index('timestamp', inplace=True)\n",
    "\n",
    "# X is everything except the new_signal column\n",
    "X = df.drop('new_signal', axis=1)\n",
    "\n",
    "# We should use the .shift() function so that our algorithm predicts the minute before realtime\n",
    "# Drop the row with NaN values \n",
    "X = X.shift().dropna()\n",
    "\n",
    "# y is the NEW signal column\n",
    "y = df[(\"new_signal\")]\n",
    "\n",
    "# Set start of training period\n",
    "training_begin = X.index.min()\n",
    "\n",
    "# we will train on 9 months and then test with the rest\n",
    "training_end = X.index.min() + DateOffset(months=9)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the X_train and y_train DataFrames\n",
    "X_train = X.loc[training_begin:training_end]\n",
    "y_train = y.loc[training_begin:training_end]\n",
    "\n",
    "# Generate the X_test and y_test DataFrames\n",
    "X_test = X.loc[training_end:]\n",
    "y_test = y.loc[training_end:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11440, 98)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(4046, 98)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a StandardScaler instance\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Apply the scaler model to fit the X-train data\n",
    "X_scaler = scaler.fit(X_train)\n",
    "\n",
    "# Transform the X_train and X_test DataFrames using the X_scaler\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "display(X_train_scaled.shape)\n",
    "display(X_test_scaled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_predictors = len(X.columns)\n",
    "\n",
    "# We have 2 possible outcomes, and we are trying to predict the stock/indicators to be in position -1 or 1\n",
    "num_classes = 1\n",
    "\n",
    "num_predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add dense layer(s)\n",
    "nn_model.add(Dense(units=16, input_dim=num_predictors, activation='relu'))\n",
    "nn_model.add(Dense(units=32, activation='relu'))\n",
    "nn_model.add(Dense(units=64, activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop-out layer(s)\n",
    "# nn_model.add(Dropout(.2,input_shape=(10,)))\n",
    "\n",
    "# Add dense layer, add Regularization\n",
    "#model.add(Dense(5, activation='relu', kernel_regularized=l2(0.01), bias_regularized=l2(0.01)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add output layer\n",
    "# Number of outputs equals number of classes\n",
    "#nn_model.add(Dense(num_classes))\n",
    "nn_model.add(Dense(num_classes, activation=\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_27 (Dense)            (None, 16)                1584      \n",
      "                                                                 \n",
      " dense_28 (Dense)            (None, 32)                544       \n",
      "                                                                 \n",
      " dense_29 (Dense)            (None, 64)                2112      \n",
      "                                                                 \n",
      " dense_30 (Dense)            (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4305 (16.82 KB)\n",
      "Trainable params: 4305 (16.82 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Compile model\n",
    "nn_model.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer=\"adam\",\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Summarize model\n",
    "nn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "286/286 [==============================] - 0s 899us/step - loss: -13.6107 - accuracy: 0.0096 - val_loss: -78.2549 - val_accuracy: 0.0372\n",
      "Epoch 2/100\n",
      "286/286 [==============================] - 0s 603us/step - loss: -794.4243 - accuracy: 0.0799 - val_loss: -2299.3806 - val_accuracy: 0.0603\n",
      "Epoch 3/100\n",
      "286/286 [==============================] - 0s 631us/step - loss: -7038.9224 - accuracy: 0.0865 - val_loss: -13472.2051 - val_accuracy: 0.1040\n",
      "Epoch 4/100\n",
      "286/286 [==============================] - 0s 631us/step - loss: -29496.8594 - accuracy: 0.0927 - val_loss: -47731.2422 - val_accuracy: 0.1088\n",
      "Epoch 5/100\n",
      "286/286 [==============================] - 0s 594us/step - loss: -83348.7891 - accuracy: 0.1039 - val_loss: -116479.4609 - val_accuracy: 0.1119\n",
      "Epoch 6/100\n",
      "286/286 [==============================] - 0s 592us/step - loss: -181704.0781 - accuracy: 0.1003 - val_loss: -233927.9688 - val_accuracy: 0.1364\n",
      "Epoch 7/100\n",
      "286/286 [==============================] - 0s 600us/step - loss: -336624.8125 - accuracy: 0.1145 - val_loss: -402517.7188 - val_accuracy: 0.1403\n",
      "Epoch 8/100\n",
      "286/286 [==============================] - 0s 660us/step - loss: -559217.7500 - accuracy: 0.1108 - val_loss: -643421.1875 - val_accuracy: 0.1390\n",
      "Epoch 9/100\n",
      "286/286 [==============================] - 0s 616us/step - loss: -865211.3750 - accuracy: 0.1164 - val_loss: -956423.8125 - val_accuracy: 0.1351\n",
      "Epoch 10/100\n",
      "286/286 [==============================] - 0s 599us/step - loss: -1270720.0000 - accuracy: 0.1187 - val_loss: -1383118.0000 - val_accuracy: 0.1469\n",
      "Epoch 11/100\n",
      "286/286 [==============================] - 0s 636us/step - loss: -1775880.3750 - accuracy: 0.1141 - val_loss: -1904388.2500 - val_accuracy: 0.1451\n",
      "Epoch 12/100\n",
      "286/286 [==============================] - 0s 611us/step - loss: -2408429.7500 - accuracy: 0.1131 - val_loss: -2531040.5000 - val_accuracy: 0.1385\n",
      "Epoch 13/100\n",
      "286/286 [==============================] - 0s 770us/step - loss: -3177701.5000 - accuracy: 0.1169 - val_loss: -3288294.5000 - val_accuracy: 0.1495\n",
      "Epoch 14/100\n",
      "286/286 [==============================] - 0s 611us/step - loss: -4096616.7500 - accuracy: 0.1171 - val_loss: -4205978.0000 - val_accuracy: 0.1420\n",
      "Epoch 15/100\n",
      "286/286 [==============================] - 0s 598us/step - loss: -5188504.5000 - accuracy: 0.1172 - val_loss: -5261726.0000 - val_accuracy: 0.1455\n",
      "Epoch 16/100\n",
      "286/286 [==============================] - 0s 595us/step - loss: -6457605.0000 - accuracy: 0.1135 - val_loss: -6503421.5000 - val_accuracy: 0.1394\n",
      "Epoch 17/100\n",
      "286/286 [==============================] - 0s 591us/step - loss: -7943046.5000 - accuracy: 0.1129 - val_loss: -7953066.0000 - val_accuracy: 0.1455\n",
      "Epoch 18/100\n",
      "286/286 [==============================] - 0s 591us/step - loss: -9636613.0000 - accuracy: 0.1146 - val_loss: -9589224.0000 - val_accuracy: 0.1399\n",
      "Epoch 19/100\n",
      "286/286 [==============================] - 0s 593us/step - loss: -11573878.0000 - accuracy: 0.1143 - val_loss: -11481842.0000 - val_accuracy: 0.1394\n",
      "Epoch 20/100\n",
      "286/286 [==============================] - 0s 646us/step - loss: -13758709.0000 - accuracy: 0.1136 - val_loss: -13575642.0000 - val_accuracy: 0.1447\n",
      "Epoch 21/100\n",
      "286/286 [==============================] - 0s 594us/step - loss: -16212475.0000 - accuracy: 0.1159 - val_loss: -15862308.0000 - val_accuracy: 0.1403\n",
      "Epoch 22/100\n",
      "286/286 [==============================] - 0s 601us/step - loss: -18907920.0000 - accuracy: 0.1136 - val_loss: -18420844.0000 - val_accuracy: 0.1451\n",
      "Epoch 23/100\n",
      "286/286 [==============================] - 0s 592us/step - loss: -21852902.0000 - accuracy: 0.1131 - val_loss: -21211162.0000 - val_accuracy: 0.1442\n",
      "Epoch 24/100\n",
      "286/286 [==============================] - 0s 590us/step - loss: -25103132.0000 - accuracy: 0.1146 - val_loss: -24240020.0000 - val_accuracy: 0.1438\n",
      "Epoch 25/100\n",
      "286/286 [==============================] - 0s 592us/step - loss: -28635930.0000 - accuracy: 0.1164 - val_loss: -27547742.0000 - val_accuracy: 0.1407\n",
      "Epoch 26/100\n",
      "286/286 [==============================] - 0s 743us/step - loss: -32506554.0000 - accuracy: 0.1132 - val_loss: -31184714.0000 - val_accuracy: 0.1377\n",
      "Epoch 27/100\n",
      "286/286 [==============================] - 0s 643us/step - loss: -36732384.0000 - accuracy: 0.1142 - val_loss: -35140000.0000 - val_accuracy: 0.1416\n",
      "Epoch 28/100\n",
      "286/286 [==============================] - 0s 658us/step - loss: -41353884.0000 - accuracy: 0.1163 - val_loss: -39492236.0000 - val_accuracy: 0.1403\n",
      "Epoch 29/100\n",
      "286/286 [==============================] - 0s 586us/step - loss: -46400488.0000 - accuracy: 0.1149 - val_loss: -44190472.0000 - val_accuracy: 0.1385\n",
      "Epoch 30/100\n",
      "286/286 [==============================] - 0s 626us/step - loss: -51781664.0000 - accuracy: 0.1133 - val_loss: -49209988.0000 - val_accuracy: 0.1394\n",
      "Epoch 31/100\n",
      "286/286 [==============================] - 0s 600us/step - loss: -57536700.0000 - accuracy: 0.1131 - val_loss: -54601312.0000 - val_accuracy: 0.1399\n",
      "Epoch 32/100\n",
      "286/286 [==============================] - 0s 670us/step - loss: -63749592.0000 - accuracy: 0.1155 - val_loss: -60386468.0000 - val_accuracy: 0.1429\n",
      "Epoch 33/100\n",
      "286/286 [==============================] - 0s 648us/step - loss: -70467208.0000 - accuracy: 0.1142 - val_loss: -66557796.0000 - val_accuracy: 0.1390\n",
      "Epoch 34/100\n",
      "286/286 [==============================] - 0s 664us/step - loss: -77621304.0000 - accuracy: 0.1125 - val_loss: -73182200.0000 - val_accuracy: 0.1403\n",
      "Epoch 35/100\n",
      "286/286 [==============================] - 0s 702us/step - loss: -85324016.0000 - accuracy: 0.1144 - val_loss: -80328512.0000 - val_accuracy: 0.1434\n",
      "Epoch 36/100\n",
      "286/286 [==============================] - 0s 626us/step - loss: -93510112.0000 - accuracy: 0.1140 - val_loss: -87761864.0000 - val_accuracy: 0.1434\n",
      "Epoch 37/100\n",
      "286/286 [==============================] - 0s 610us/step - loss: -102139448.0000 - accuracy: 0.1139 - val_loss: -95820744.0000 - val_accuracy: 0.1407\n",
      "Epoch 38/100\n",
      "286/286 [==============================] - 0s 621us/step - loss: -111327064.0000 - accuracy: 0.1146 - val_loss: -104114808.0000 - val_accuracy: 0.1416\n",
      "Epoch 39/100\n",
      "286/286 [==============================] - 0s 742us/step - loss: -121069952.0000 - accuracy: 0.1148 - val_loss: -113162528.0000 - val_accuracy: 0.1425\n",
      "Epoch 40/100\n",
      "286/286 [==============================] - 0s 620us/step - loss: -131437416.0000 - accuracy: 0.1143 - val_loss: -122657512.0000 - val_accuracy: 0.1434\n",
      "Epoch 41/100\n",
      "286/286 [==============================] - 0s 610us/step - loss: -142354864.0000 - accuracy: 0.1144 - val_loss: -132845296.0000 - val_accuracy: 0.1434\n",
      "Epoch 42/100\n",
      "286/286 [==============================] - 0s 608us/step - loss: -153918352.0000 - accuracy: 0.1157 - val_loss: -143524960.0000 - val_accuracy: 0.1420\n",
      "Epoch 43/100\n",
      "286/286 [==============================] - 0s 932us/step - loss: -166214720.0000 - accuracy: 0.1156 - val_loss: -154869504.0000 - val_accuracy: 0.1403\n",
      "Epoch 44/100\n",
      "286/286 [==============================] - 0s 903us/step - loss: -179295504.0000 - accuracy: 0.1143 - val_loss: -166979440.0000 - val_accuracy: 0.1420\n",
      "Epoch 45/100\n",
      "286/286 [==============================] - 0s 777us/step - loss: -193159200.0000 - accuracy: 0.1139 - val_loss: -179576432.0000 - val_accuracy: 0.1420\n",
      "Epoch 46/100\n",
      "286/286 [==============================] - 0s 923us/step - loss: -207666112.0000 - accuracy: 0.1148 - val_loss: -192674784.0000 - val_accuracy: 0.1429\n",
      "Epoch 47/100\n",
      "286/286 [==============================] - 0s 738us/step - loss: -222918416.0000 - accuracy: 0.1151 - val_loss: -206682160.0000 - val_accuracy: 0.1425\n",
      "Epoch 48/100\n",
      "286/286 [==============================] - 0s 632us/step - loss: -238970768.0000 - accuracy: 0.1151 - val_loss: -221321936.0000 - val_accuracy: 0.1403\n",
      "Epoch 49/100\n",
      "286/286 [==============================] - 0s 611us/step - loss: -255660864.0000 - accuracy: 0.1140 - val_loss: -236567104.0000 - val_accuracy: 0.1416\n",
      "Epoch 50/100\n",
      "286/286 [==============================] - 0s 601us/step - loss: -273254592.0000 - accuracy: 0.1156 - val_loss: -252460304.0000 - val_accuracy: 0.1420\n",
      "Epoch 51/100\n",
      "286/286 [==============================] - 0s 705us/step - loss: -291580992.0000 - accuracy: 0.1145 - val_loss: -269468000.0000 - val_accuracy: 0.1412\n",
      "Epoch 52/100\n",
      "286/286 [==============================] - 0s 677us/step - loss: -310849088.0000 - accuracy: 0.1146 - val_loss: -287226304.0000 - val_accuracy: 0.1420\n",
      "Epoch 53/100\n",
      "286/286 [==============================] - 0s 679us/step - loss: -331138560.0000 - accuracy: 0.1133 - val_loss: -305472128.0000 - val_accuracy: 0.1425\n",
      "Epoch 54/100\n",
      "286/286 [==============================] - 0s 656us/step - loss: -352055104.0000 - accuracy: 0.1158 - val_loss: -324976928.0000 - val_accuracy: 0.1425\n",
      "Epoch 55/100\n",
      "286/286 [==============================] - 0s 605us/step - loss: -373991424.0000 - accuracy: 0.1145 - val_loss: -344944512.0000 - val_accuracy: 0.1425\n",
      "Epoch 56/100\n",
      "286/286 [==============================] - 0s 965us/step - loss: -396986624.0000 - accuracy: 0.1142 - val_loss: -365227648.0000 - val_accuracy: 0.1429\n",
      "Epoch 57/100\n",
      "286/286 [==============================] - 0s 777us/step - loss: -420955776.0000 - accuracy: 0.1144 - val_loss: -386927360.0000 - val_accuracy: 0.1425\n",
      "Epoch 58/100\n",
      "286/286 [==============================] - 0s 597us/step - loss: -445972384.0000 - accuracy: 0.1137 - val_loss: -409373216.0000 - val_accuracy: 0.1425\n",
      "Epoch 59/100\n",
      "286/286 [==============================] - 0s 739us/step - loss: -472252704.0000 - accuracy: 0.1146 - val_loss: -432869248.0000 - val_accuracy: 0.1429\n",
      "Epoch 60/100\n",
      "286/286 [==============================] - 0s 687us/step - loss: -499485664.0000 - accuracy: 0.1141 - val_loss: -457571008.0000 - val_accuracy: 0.1429\n",
      "Epoch 61/100\n",
      "286/286 [==============================] - 0s 717us/step - loss: -527880064.0000 - accuracy: 0.1147 - val_loss: -483764640.0000 - val_accuracy: 0.1438\n",
      "Epoch 62/100\n",
      "286/286 [==============================] - 0s 667us/step - loss: -557239680.0000 - accuracy: 0.1147 - val_loss: -509459456.0000 - val_accuracy: 0.1425\n",
      "Epoch 63/100\n",
      "286/286 [==============================] - 0s 632us/step - loss: -587738048.0000 - accuracy: 0.1146 - val_loss: -537361920.0000 - val_accuracy: 0.1438\n",
      "Epoch 64/100\n",
      "286/286 [==============================] - 0s 596us/step - loss: -619284544.0000 - accuracy: 0.1144 - val_loss: -565244032.0000 - val_accuracy: 0.1420\n",
      "Epoch 65/100\n",
      "286/286 [==============================] - 0s 668us/step - loss: -652022336.0000 - accuracy: 0.1141 - val_loss: -594627328.0000 - val_accuracy: 0.1425\n",
      "Epoch 66/100\n",
      "286/286 [==============================] - 0s 624us/step - loss: -686170624.0000 - accuracy: 0.1149 - val_loss: -625820544.0000 - val_accuracy: 0.1425\n",
      "Epoch 67/100\n",
      "286/286 [==============================] - 0s 594us/step - loss: -721752320.0000 - accuracy: 0.1145 - val_loss: -657664256.0000 - val_accuracy: 0.1429\n",
      "Epoch 68/100\n",
      "286/286 [==============================] - 0s 611us/step - loss: -758904832.0000 - accuracy: 0.1147 - val_loss: -690336128.0000 - val_accuracy: 0.1425\n",
      "Epoch 69/100\n",
      "286/286 [==============================] - 0s 670us/step - loss: -796926848.0000 - accuracy: 0.1145 - val_loss: -724807104.0000 - val_accuracy: 0.1429\n",
      "Epoch 70/100\n",
      "286/286 [==============================] - 0s 774us/step - loss: -836484800.0000 - accuracy: 0.1152 - val_loss: -759902720.0000 - val_accuracy: 0.1429\n",
      "Epoch 71/100\n",
      "286/286 [==============================] - 0s 621us/step - loss: -877416960.0000 - accuracy: 0.1149 - val_loss: -796364608.0000 - val_accuracy: 0.1429\n",
      "Epoch 72/100\n",
      "286/286 [==============================] - 0s 590us/step - loss: -919942720.0000 - accuracy: 0.1140 - val_loss: -834249856.0000 - val_accuracy: 0.1434\n",
      "Epoch 73/100\n",
      "286/286 [==============================] - 0s 593us/step - loss: -963895040.0000 - accuracy: 0.1154 - val_loss: -873218560.0000 - val_accuracy: 0.1420\n",
      "Epoch 74/100\n",
      "286/286 [==============================] - 0s 591us/step - loss: -1009153600.0000 - accuracy: 0.1148 - val_loss: -914599872.0000 - val_accuracy: 0.1425\n",
      "Epoch 75/100\n",
      "286/286 [==============================] - 0s 592us/step - loss: -1055929920.0000 - accuracy: 0.1153 - val_loss: -955688320.0000 - val_accuracy: 0.1425\n",
      "Epoch 76/100\n",
      "286/286 [==============================] - 0s 587us/step - loss: -1104237696.0000 - accuracy: 0.1139 - val_loss: -998205952.0000 - val_accuracy: 0.1425\n",
      "Epoch 77/100\n",
      "286/286 [==============================] - 0s 587us/step - loss: -1153543424.0000 - accuracy: 0.1145 - val_loss: -1042261632.0000 - val_accuracy: 0.1425\n",
      "Epoch 78/100\n",
      "286/286 [==============================] - 0s 597us/step - loss: -1204417152.0000 - accuracy: 0.1152 - val_loss: -1087115520.0000 - val_accuracy: 0.1425\n",
      "Epoch 79/100\n",
      "286/286 [==============================] - 0s 632us/step - loss: -1256883968.0000 - accuracy: 0.1146 - val_loss: -1134287488.0000 - val_accuracy: 0.1425\n",
      "Epoch 80/100\n",
      "286/286 [==============================] - 0s 590us/step - loss: -1311126912.0000 - accuracy: 0.1149 - val_loss: -1182647424.0000 - val_accuracy: 0.1429\n",
      "Epoch 81/100\n",
      "286/286 [==============================] - 0s 681us/step - loss: -1367150464.0000 - accuracy: 0.1153 - val_loss: -1232300544.0000 - val_accuracy: 0.1425\n",
      "Epoch 82/100\n",
      "286/286 [==============================] - 0s 590us/step - loss: -1425772672.0000 - accuracy: 0.1153 - val_loss: -1284725632.0000 - val_accuracy: 0.1425\n",
      "Epoch 83/100\n",
      "286/286 [==============================] - 0s 592us/step - loss: -1485788288.0000 - accuracy: 0.1148 - val_loss: -1336912768.0000 - val_accuracy: 0.1429\n",
      "Epoch 84/100\n",
      "286/286 [==============================] - 0s 594us/step - loss: -1547890304.0000 - accuracy: 0.1151 - val_loss: -1390683392.0000 - val_accuracy: 0.1447\n",
      "Epoch 85/100\n",
      "286/286 [==============================] - 0s 597us/step - loss: -1612006656.0000 - accuracy: 0.1151 - val_loss: -1447079936.0000 - val_accuracy: 0.1429\n",
      "Epoch 86/100\n",
      "286/286 [==============================] - 0s 613us/step - loss: -1678248576.0000 - accuracy: 0.1149 - val_loss: -1506883968.0000 - val_accuracy: 0.1429\n",
      "Epoch 87/100\n",
      "286/286 [==============================] - 0s 593us/step - loss: -1745839616.0000 - accuracy: 0.1156 - val_loss: -1566225280.0000 - val_accuracy: 0.1434\n",
      "Epoch 88/100\n",
      "286/286 [==============================] - 0s 590us/step - loss: -1815378688.0000 - accuracy: 0.1149 - val_loss: -1627704320.0000 - val_accuracy: 0.1447\n",
      "Epoch 89/100\n",
      "286/286 [==============================] - 0s 593us/step - loss: -1886697216.0000 - accuracy: 0.1148 - val_loss: -1690752256.0000 - val_accuracy: 0.1429\n",
      "Epoch 90/100\n",
      "286/286 [==============================] - 0s 592us/step - loss: -1959937536.0000 - accuracy: 0.1149 - val_loss: -1755410944.0000 - val_accuracy: 0.1434\n",
      "Epoch 91/100\n",
      "286/286 [==============================] - 0s 588us/step - loss: -2035181568.0000 - accuracy: 0.1146 - val_loss: -1822150784.0000 - val_accuracy: 0.1429\n",
      "Epoch 92/100\n",
      "286/286 [==============================] - 0s 749us/step - loss: -2112994816.0000 - accuracy: 0.1144 - val_loss: -1890288128.0000 - val_accuracy: 0.1438\n",
      "Epoch 93/100\n",
      "286/286 [==============================] - 0s 588us/step - loss: -2193320192.0000 - accuracy: 0.1148 - val_loss: -1961577984.0000 - val_accuracy: 0.1460\n",
      "Epoch 94/100\n",
      "286/286 [==============================] - 0s 595us/step - loss: -2276424704.0000 - accuracy: 0.1153 - val_loss: -2034003200.0000 - val_accuracy: 0.1460\n",
      "Epoch 95/100\n",
      "286/286 [==============================] - 0s 589us/step - loss: -2361660416.0000 - accuracy: 0.1149 - val_loss: -2109245696.0000 - val_accuracy: 0.1455\n",
      "Epoch 96/100\n",
      "286/286 [==============================] - 0s 593us/step - loss: -2448663808.0000 - accuracy: 0.1147 - val_loss: -2186741760.0000 - val_accuracy: 0.1460\n",
      "Epoch 97/100\n",
      "286/286 [==============================] - 0s 581us/step - loss: -2537393920.0000 - accuracy: 0.1153 - val_loss: -2264420608.0000 - val_accuracy: 0.1455\n",
      "Epoch 98/100\n",
      "286/286 [==============================] - 0s 589us/step - loss: -2628378624.0000 - accuracy: 0.1151 - val_loss: -2344497152.0000 - val_accuracy: 0.1451\n",
      "Epoch 99/100\n",
      "286/286 [==============================] - 0s 588us/step - loss: -2721483008.0000 - accuracy: 0.1146 - val_loss: -2427467008.0000 - val_accuracy: 0.1460\n",
      "Epoch 100/100\n",
      "286/286 [==============================] - 0s 610us/step - loss: -2818124032.0000 - accuracy: 0.1149 - val_loss: -2514242816.0000 - val_accuracy: 0.1473\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x29be3d3d0>"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit model\n",
    "num_epochs = 100\n",
    "\n",
    "nn_model.fit(X_train_scaled, y_train,\n",
    "          epochs=num_epochs,\n",
    "          batch_size=32,\n",
    "          validation_split=0.2,     # This 'validation_split' is telling the neural network to keep 20% of the data to validate its score on the training set... this is to help AVOID OVERFITTING. \n",
    "          shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "127/127 - 0s - loss: -2.8338e+09 - accuracy: 0.0976 - 102ms/epoch - 800us/step\n",
      "Loss: -2833773824.0, Accuracy: 0.09762728959321976\n"
     ]
    }
   ],
   "source": [
    "# Show model loss and accuracy\n",
    "\n",
    "# Evaluate the model loss and accuracy metrics using the evaluate method and the test data\n",
    "model_loss, model_accuracy = nn_model.evaluate(X_test_scaled, y_test, verbose=2)\n",
    "\n",
    "# Display the evaluation results\n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "127/127 [==============================] - 0s 334us/step\n",
      "358/358 [==============================] - 0s 271us/step\n"
     ]
    }
   ],
   "source": [
    "# back test\n",
    "# Predict values using testing data\n",
    "nn_test_predictions = nn_model.predict(X_test_scaled)\n",
    "nn_train_predictions = nn_model.predict(X_train_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXTRA\n",
    "\n",
    "\n",
    "\n",
    "# # Evaluate the model using a classification report\n",
    "# from sklearn.metrics import classification_report\n",
    "# training_report = classification_report(nn_train_predictions, nn_test_predictions)\n",
    "# print(training_report)\n",
    "\n",
    "# # Evaluate the model's ability to predict the trading signal for the testing data using a classification report\n",
    "# training_report = classification_report(y_test, testing_signal_predictions)\n",
    "# print(training_report)\n",
    "\n",
    "# from imblearn.over_sampling import RandomOverSampler\n",
    "# # Use RandomOverSampler to resample the datase using random_state=1\n",
    "# ros = RandomOverSampler(random_state=1)\n",
    "# X_resampled, y_resampled = ros.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "# Drop-out layer(s)\n",
    "# nn_model.add(Dropout(.2,input_shape=(10,)))\n",
    "\n",
    "# Add dense layer, add Regularization\n",
    "#model.add(Dense(5, activation='relu', kernel_regularized=l2(0.01), bias_regularized=l2(0.01)))\n",
    "\n",
    "# # Save model history for further manipulation\n",
    "# model_history = model.history.model_history.keys()\n",
    "\n",
    "# # Now we can plot the accuracy for training and validation\n",
    "\n",
    "# training_results = pd.DataFrame(index=range(1, num_epochs+1))\n",
    "# training_results['Training'] = model_history['categorical_accuracy']\n",
    "# training_results['Validation'] = model_history['val_categorical_accuracy']\n",
    "# training_results.plot(title = 'Training and Validation Performance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11440, 98)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(4046, 98)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(X_train_scaled.shape)\n",
    "display(X_test_scaled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11440"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "98"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_samples = X_train_scaled.shape[0]\n",
    "num_features = X_train_scaled.shape[1]\n",
    "\n",
    "display(num_samples)\n",
    "display(num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_timesteps = 1  # This is an example; you might need a different value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape X_train_scaled and X_test_scaled\n",
    "X_train_reshaped = X_train_scaled.reshape((num_samples, num_timesteps, num_features))\n",
    "X_test_reshaped = X_test_scaled.reshape((X_test_scaled.shape[0], num_timesteps, num_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-2.41770309e+00, -2.42670149e+00, -2.40594906e+00, ...,\n",
       "          1.81155121e+00, -1.11872189e+00, -1.35846314e-03]],\n",
       "\n",
       "       [[-2.41658341e+00, -2.42731372e+00, -2.40228144e+00, ...,\n",
       "          1.75288220e+00, -1.11872189e+00, -1.35846314e-03]],\n",
       "\n",
       "       [[-2.41903080e+00, -2.42853820e+00, -2.40656033e+00, ...,\n",
       "          1.64834414e+00, -1.11872189e+00, -1.35846314e-03]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 5.67395127e-01,  5.63464862e-01,  5.75220682e-01, ...,\n",
       "          4.57471385e-01,  1.12130670e+00, -2.22147536e+00]],\n",
       "\n",
       "       [[ 5.69230669e-01,  5.57954728e-01,  5.78888310e-01, ...,\n",
       "          1.88311378e-01,  1.09079146e+00, -1.35846314e-03]],\n",
       "\n",
       "       [[ 5.66171433e-01,  5.56730254e-01,  5.75831953e-01, ...,\n",
       "          5.26666840e-02,  1.00714243e+00, -1.35846314e-03]]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[[ 5.66171433e-01,  5.56730254e-01,  5.75831953e-01, ...,\n",
       "          5.26666840e-02,  1.00714243e+00, -1.35846314e-03]],\n",
       "\n",
       "       [[ 5.64335891e-01,  5.54281305e-01,  5.75220682e-01, ...,\n",
       "         -1.48337922e-01,  7.88218309e-01, -1.35846314e-03]],\n",
       "\n",
       "       [[ 5.63112197e-01,  5.56118016e-01,  5.74609411e-01, ...,\n",
       "         -3.97701095e-01,  3.56310828e-01, -1.35846314e-03]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 8.32324942e-01,  8.29175795e-01,  8.38678579e-01, ...,\n",
       "         -2.55252057e-01,  1.12156143e+00,  2.21875844e+00]],\n",
       "\n",
       "       [[ 8.29877553e-01,  8.24890135e-01,  8.39289850e-01, ...,\n",
       "         -5.50127696e-01,  1.12156143e+00, -1.35846314e-03]],\n",
       "\n",
       "       [[ 8.23147235e-01,  8.23665661e-01,  8.34399680e-01, ...,\n",
       "         -7.15284152e-01,  1.11778458e+00, -1.35846314e-03]]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(X_train_reshaped)\n",
    "display(X_test_reshaped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_reshaped.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_reshaped.shape[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trash model, let's change some stuff up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_4 (LSTM)               (None, 1, 50)             29800     \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 1, 50)             0         \n",
      "                                                                 \n",
      " lstm_5 (LSTM)               (None, 50)                20200     \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 50)                0         \n",
      "                                                                 \n",
      " dense_31 (Dense)            (None, 64)                3264      \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_32 (Dense)            (None, 1)                 65        \n",
      "                                                                 \n",
      " dense_33 (Dense)            (None, 1)                 2         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 53331 (208.32 KB)\n",
      "Trainable params: 53331 (208.32 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/Users/montygash/anaconda3/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"/Users/montygash/anaconda3/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/Users/montygash/anaconda3/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"/Users/montygash/anaconda3/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1150, in train_step\n        y_pred = self(x, training=True)\n    File \"/Users/montygash/anaconda3/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/Users/montygash/anaconda3/lib/python3.11/site-packages/keras/src/engine/input_spec.py\", line 298, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"sequential_12\" is incompatible with the layer: expected shape=(None, 1, 98), found shape=(32, 98)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[185], line 39\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# Fit model\u001b[39;00m\n\u001b[1;32m     38\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m50\u001b[39m\n\u001b[0;32m---> 39\u001b[0m nn_model\u001b[38;5;241m.\u001b[39mfit(X_train_scaled, y_train, epochs\u001b[38;5;241m=\u001b[39mnum_epochs, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, validation_split\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/var/folders/tl/pmdc2_wx3hx1zkjc2fzswyyc0000gn/T/__autograph_generated_filem0sllk_g.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/Users/montygash/anaconda3/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"/Users/montygash/anaconda3/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/Users/montygash/anaconda3/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"/Users/montygash/anaconda3/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1150, in train_step\n        y_pred = self(x, training=True)\n    File \"/Users/montygash/anaconda3/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/Users/montygash/anaconda3/lib/python3.11/site-packages/keras/src/engine/input_spec.py\", line 298, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"sequential_12\" is incompatible with the layer: expected shape=(None, 1, 98), found shape=(32, 98)\n"
     ]
    }
   ],
   "source": [
    "num_predictors = len(X.columns)\n",
    "\n",
    "# We have 2 possible outcomes, and we are trying to predict the stock/indicators to be in position -1 or 1\n",
    "num_classes = 1\n",
    "\n",
    "nn_model = Sequential()\n",
    "\n",
    "# Add LSTM layer\n",
    "nn_model.add(LSTM(units=50, return_sequences=True, input_shape=(X_train_reshaped.shape[1], X_train_reshaped.shape[2])))\n",
    "nn_model.add(Dropout(0.2))\n",
    "\n",
    "# Add another LSTM layer\n",
    "nn_model.add(LSTM(units=50, return_sequences=False))\n",
    "nn_model.add(Dropout(0.2))\n",
    "\n",
    "# Add dense layers\n",
    "nn_model.add(Dense(units=64, activation='relu'))\n",
    "nn_model.add(Dropout(0.2))\n",
    "\n",
    "# Add output layer\n",
    "# Number of outputs equals number of classes\n",
    "#nn_model.add(Dense(num_classes))\n",
    "nn_model.add(Dense(num_classes, activation=\"sigmoid\"))\n",
    "\n",
    "# Add output layer\n",
    "nn_model.add(Dense(num_classes, activation=\"sigmoid\"))\n",
    "\n",
    "# Compile model\n",
    "nn_model.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer=\"adam\",\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Summarize model\n",
    "nn_model.summary()\n",
    "\n",
    "\n",
    "# Fit model\n",
    "num_epochs = 50\n",
    "nn_model.fit(X_train_scaled, y_train, epochs=num_epochs, batch_size=32, validation_split=0.2, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### try removing the highly correlated things, use correlation matrix.. see Marghub's example project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_13\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_6 (LSTM)               (None, 1, 50)             29800     \n",
      "                                                                 \n",
      " dropout_9 (Dropout)         (None, 1, 50)             0         \n",
      "                                                                 \n",
      " lstm_7 (LSTM)               (None, 50)                20200     \n",
      "                                                                 \n",
      " dropout_10 (Dropout)        (None, 50)                0         \n",
      "                                                                 \n",
      " dense_34 (Dense)            (None, 64)                3264      \n",
      "                                                                 \n",
      " dropout_11 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 53264 (208.06 KB)\n",
      "Trainable params: 53264 (208.06 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "286/286 [==============================] - 2s 2ms/step - loss: -0.2478 - accuracy: 0.0042 - val_loss: -0.4910 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/50\n",
      "286/286 [==============================] - 0s 1ms/step - loss: -0.2882 - accuracy: 0.0045 - val_loss: -0.5574 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/50\n",
      "286/286 [==============================] - 0s 1ms/step - loss: -0.3081 - accuracy: 0.0016 - val_loss: -0.5767 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/50\n",
      "286/286 [==============================] - 0s 1ms/step - loss: -0.3394 - accuracy: 2.1853e-04 - val_loss: -0.7378 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/50\n",
      "286/286 [==============================] - 0s 1ms/step - loss: -0.3945 - accuracy: 1.0927e-04 - val_loss: -0.8915 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/50\n",
      "286/286 [==============================] - 0s 1ms/step - loss: -0.4449 - accuracy: 2.1853e-04 - val_loss: -0.9645 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/50\n",
      "286/286 [==============================] - 0s 1ms/step - loss: -0.4626 - accuracy: 0.0000e+00 - val_loss: -1.0506 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/50\n",
      "286/286 [==============================] - 0s 1ms/step - loss: -0.5458 - accuracy: 0.0000e+00 - val_loss: -1.0460 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/50\n",
      "286/286 [==============================] - 1s 2ms/step - loss: -0.5282 - accuracy: 0.0000e+00 - val_loss: -1.1243 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/50\n",
      "286/286 [==============================] - 0s 1ms/step - loss: -0.6617 - accuracy: 0.0000e+00 - val_loss: -1.2283 - val_accuracy: 0.0000e+00\n",
      "Epoch 11/50\n",
      "286/286 [==============================] - 0s 1ms/step - loss: -0.7316 - accuracy: 0.0000e+00 - val_loss: -1.3090 - val_accuracy: 0.0000e+00\n",
      "Epoch 12/50\n",
      "286/286 [==============================] - 0s 1ms/step - loss: -0.7854 - accuracy: 0.0000e+00 - val_loss: -1.4106 - val_accuracy: 0.0000e+00\n",
      "Epoch 13/50\n",
      "286/286 [==============================] - 0s 1ms/step - loss: -0.8319 - accuracy: 0.0000e+00 - val_loss: -1.4205 - val_accuracy: 0.0000e+00\n",
      "Epoch 14/50\n",
      "286/286 [==============================] - 0s 1ms/step - loss: -0.9380 - accuracy: 0.0000e+00 - val_loss: -1.6304 - val_accuracy: 0.0000e+00\n",
      "Epoch 15/50\n",
      "286/286 [==============================] - 0s 1ms/step - loss: -1.0495 - accuracy: 0.0000e+00 - val_loss: -1.7406 - val_accuracy: 0.0000e+00\n",
      "Epoch 16/50\n",
      "286/286 [==============================] - 0s 1ms/step - loss: -1.1407 - accuracy: 0.0000e+00 - val_loss: -1.8178 - val_accuracy: 0.0000e+00\n",
      "Epoch 17/50\n",
      "286/286 [==============================] - 0s 1ms/step - loss: -1.1875 - accuracy: 9.8339e-04 - val_loss: -1.8623 - val_accuracy: 0.0000e+00\n",
      "Epoch 18/50\n",
      "286/286 [==============================] - 0s 1ms/step - loss: -1.2185 - accuracy: 3.2780e-04 - val_loss: -1.9307 - val_accuracy: 0.0000e+00\n",
      "Epoch 19/50\n",
      "286/286 [==============================] - 0s 1ms/step - loss: -1.2258 - accuracy: 2.1853e-04 - val_loss: -1.9195 - val_accuracy: 0.0000e+00\n",
      "Epoch 20/50\n",
      "286/286 [==============================] - 0s 1ms/step - loss: -1.2730 - accuracy: 0.0000e+00 - val_loss: -2.0150 - val_accuracy: 0.0000e+00\n",
      "Epoch 21/50\n",
      "286/286 [==============================] - 0s 1ms/step - loss: -1.2117 - accuracy: 2.1853e-04 - val_loss: -2.0656 - val_accuracy: 0.0000e+00\n",
      "Epoch 22/50\n",
      "286/286 [==============================] - 0s 1ms/step - loss: -1.2243 - accuracy: 2.1853e-04 - val_loss: -1.9055 - val_accuracy: 0.0000e+00\n",
      "Epoch 23/50\n",
      "286/286 [==============================] - 0s 1ms/step - loss: -1.4305 - accuracy: 2.1853e-04 - val_loss: -2.0174 - val_accuracy: 0.0000e+00\n",
      "Epoch 24/50\n",
      "286/286 [==============================] - 0s 1ms/step - loss: -1.5075 - accuracy: 8.7413e-04 - val_loss: -2.1768 - val_accuracy: 0.0000e+00\n",
      "Epoch 25/50\n",
      "286/286 [==============================] - 0s 1ms/step - loss: -1.5097 - accuracy: 3.2780e-04 - val_loss: -2.1201 - val_accuracy: 0.0000e+00\n",
      "Epoch 26/50\n",
      "286/286 [==============================] - 0s 1ms/step - loss: -1.6319 - accuracy: 2.1853e-04 - val_loss: -2.1332 - val_accuracy: 0.0000e+00\n",
      "Epoch 27/50\n",
      "286/286 [==============================] - 0s 1ms/step - loss: -1.6608 - accuracy: 4.3706e-04 - val_loss: -2.1449 - val_accuracy: 0.0000e+00\n",
      "Epoch 28/50\n",
      "286/286 [==============================] - 0s 1ms/step - loss: -1.6951 - accuracy: 0.0019 - val_loss: -2.2870 - val_accuracy: 4.3706e-04\n",
      "Epoch 29/50\n",
      "286/286 [==============================] - 0s 1ms/step - loss: -1.7247 - accuracy: 0.0032 - val_loss: -2.1941 - val_accuracy: 0.0000e+00\n",
      "Epoch 30/50\n",
      "286/286 [==============================] - 0s 1ms/step - loss: -1.4529 - accuracy: 6.5559e-04 - val_loss: -2.0377 - val_accuracy: 0.0000e+00\n",
      "Epoch 31/50\n",
      "286/286 [==============================] - 0s 1ms/step - loss: -1.5043 - accuracy: 0.0014 - val_loss: -1.7928 - val_accuracy: 0.0013\n",
      "Epoch 32/50\n",
      "286/286 [==============================] - 0s 1ms/step - loss: -1.4422 - accuracy: 0.0011 - val_loss: -1.9902 - val_accuracy: 0.0000e+00\n",
      "Epoch 33/50\n",
      "286/286 [==============================] - 0s 1ms/step - loss: -1.5059 - accuracy: 7.6486e-04 - val_loss: -2.0725 - val_accuracy: 0.0000e+00\n",
      "Epoch 34/50\n",
      "286/286 [==============================] - 0s 1ms/step - loss: -1.7378 - accuracy: 5.4633e-04 - val_loss: -2.1515 - val_accuracy: 0.0000e+00\n",
      "Epoch 35/50\n",
      "286/286 [==============================] - 0s 1ms/step - loss: -1.8338 - accuracy: 6.5559e-04 - val_loss: -2.1567 - val_accuracy: 0.0000e+00\n",
      "Epoch 36/50\n",
      "286/286 [==============================] - 0s 1ms/step - loss: -1.7776 - accuracy: 2.1853e-04 - val_loss: -2.0930 - val_accuracy: 0.0000e+00\n",
      "Epoch 37/50\n",
      "286/286 [==============================] - 0s 1ms/step - loss: -1.7770 - accuracy: 1.0927e-04 - val_loss: -2.1173 - val_accuracy: 0.0000e+00\n",
      "Epoch 38/50\n",
      "286/286 [==============================] - 0s 1ms/step - loss: -1.8957 - accuracy: 2.1853e-04 - val_loss: -2.2065 - val_accuracy: 0.0000e+00\n",
      "Epoch 39/50\n",
      "286/286 [==============================] - 0s 1ms/step - loss: -1.7431 - accuracy: 2.1853e-04 - val_loss: -2.1484 - val_accuracy: 0.0000e+00\n",
      "Epoch 40/50\n",
      "286/286 [==============================] - 0s 1ms/step - loss: -1.6538 - accuracy: 1.0927e-04 - val_loss: -2.2754 - val_accuracy: 0.0000e+00\n",
      "Epoch 41/50\n",
      "286/286 [==============================] - 0s 1ms/step - loss: -1.7010 - accuracy: 3.2780e-04 - val_loss: -2.2744 - val_accuracy: 0.0000e+00\n",
      "Epoch 42/50\n",
      "286/286 [==============================] - 0s 1ms/step - loss: -1.6778 - accuracy: 5.4633e-04 - val_loss: -2.2760 - val_accuracy: 0.0000e+00\n",
      "Epoch 43/50\n",
      "286/286 [==============================] - 0s 1ms/step - loss: -1.8716 - accuracy: 3.2780e-04 - val_loss: -2.2032 - val_accuracy: 0.0000e+00\n",
      "Epoch 44/50\n",
      "286/286 [==============================] - 0s 1ms/step - loss: -1.8886 - accuracy: 5.4633e-04 - val_loss: -2.1805 - val_accuracy: 0.0000e+00\n",
      "Epoch 45/50\n",
      "286/286 [==============================] - 0s 1ms/step - loss: -1.7622 - accuracy: 0.0013 - val_loss: -2.2151 - val_accuracy: 0.0000e+00\n",
      "Epoch 46/50\n",
      "286/286 [==============================] - 0s 1ms/step - loss: -1.7628 - accuracy: 0.0012 - val_loss: -2.1672 - val_accuracy: 0.0000e+00\n",
      "Epoch 47/50\n",
      "286/286 [==============================] - 0s 1ms/step - loss: -1.7714 - accuracy: 4.3706e-04 - val_loss: -2.2152 - val_accuracy: 0.0000e+00\n",
      "Epoch 48/50\n",
      "286/286 [==============================] - 0s 1ms/step - loss: -1.8011 - accuracy: 2.1853e-04 - val_loss: -2.2572 - val_accuracy: 0.0000e+00\n",
      "Epoch 49/50\n",
      "286/286 [==============================] - 0s 1ms/step - loss: -1.9413 - accuracy: 0.0000e+00 - val_loss: -2.1558 - val_accuracy: 0.0000e+00\n",
      "Epoch 50/50\n",
      "286/286 [==============================] - 0s 1ms/step - loss: -1.8436 - accuracy: 0.0000e+00 - val_loss: -2.0809 - val_accuracy: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2c7156890>"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_features = X_train_scaled.shape[1]\n",
    "\n",
    "# Reshape the data to include the time steps dimension\n",
    "# Here, we're using 1 time step as an example\n",
    "X_train_reshaped = X_train_scaled.reshape(-1, 1, num_features)\n",
    "X_test_reshaped = X_test_scaled.reshape(-1, 1, num_features)\n",
    "\n",
    "# We have 2 possible outcomes, and we are trying to predict the stock/indicators to be in position -1 or 1\n",
    "num_classes = 1\n",
    "\n",
    "nn_model = Sequential()\n",
    "\n",
    "# Add LSTM layer\n",
    "nn_model.add(LSTM(units=50, return_sequences=True, input_shape=(X_train_reshaped.shape[1], X_train_reshaped.shape[2])))\n",
    "nn_model.add(Dropout(0.2))\n",
    "\n",
    "# Add another LSTM layer\n",
    "nn_model.add(LSTM(units=50, return_sequences=False))\n",
    "nn_model.add(Dropout(0.2))\n",
    "\n",
    "# Add dense layers\n",
    "nn_model.add(Dense(units=64, activation='relu'))\n",
    "nn_model.add(Dropout(0.2))\n",
    "\n",
    "# Compile model\n",
    "nn_model.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer=\"adam\",\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Summarize model\n",
    "nn_model.summary()\n",
    "\n",
    "\n",
    "# Fit model\n",
    "num_epochs = 50\n",
    "nn_model.fit(X_train_reshaped, y_train, epochs=num_epochs, batch_size=32, validation_split=0.2, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change time step from 1 to more.\n",
    "\n",
    "This can be calculated by dividing the total number of elements in X_train_scaled by the number of features and then by the desired number of time steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the total number of samples that can be formed with 100 time steps\n",
    "num_samples = X_train_scaled.shape[0] // 1000\n",
    "\n",
    "# Now reshape the array\n",
    "X_train_reshaped = X_train_scaled[:num_samples * 1000].reshape(num_samples, 1000, num_features)\n",
    "\n",
    "# Calculate the total number of samples that can be formed with 100 time steps\n",
    "num_samples = X_test_scaled.shape[0] // 1000\n",
    "\n",
    "# Now reshape the array\n",
    "X_train_reshaped = X_test_scaled[:num_samples * 1000].reshape(num_samples, 1000, num_features)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_15\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_10 (LSTM)              (None, 1000, 50)          29800     \n",
      "                                                                 \n",
      " dropout_15 (Dropout)        (None, 1000, 50)          0         \n",
      "                                                                 \n",
      " lstm_11 (LSTM)              (None, 50)                20200     \n",
      "                                                                 \n",
      " dropout_16 (Dropout)        (None, 50)                0         \n",
      "                                                                 \n",
      " dense_36 (Dense)            (None, 64)                3264      \n",
      "                                                                 \n",
      " dropout_17 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 53264 (208.06 KB)\n",
      "Trainable params: 53264 (208.06 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "1/1 [==============================] - 2s 2s/step - loss: 10.0870 - accuracy: 0.0000e+00 - val_loss: 8.4583 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 292ms/step - loss: 9.1227 - accuracy: 0.0000e+00 - val_loss: 7.3421 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 283ms/step - loss: 8.7876 - accuracy: 0.0000e+00 - val_loss: 6.9035 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 283ms/step - loss: 8.1627 - accuracy: 0.0000e+00 - val_loss: 6.4092 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 281ms/step - loss: 8.2288 - accuracy: 0.0000e+00 - val_loss: 6.1818 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 298ms/step - loss: 8.4210 - accuracy: 0.0000e+00 - val_loss: 6.0188 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 285ms/step - loss: 8.4751 - accuracy: 0.0000e+00 - val_loss: 5.9528 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 286ms/step - loss: 7.6980 - accuracy: 0.0000e+00 - val_loss: 5.5708 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 284ms/step - loss: 7.8943 - accuracy: 0.0000e+00 - val_loss: 5.6682 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 280ms/step - loss: 7.2420 - accuracy: 0.0000e+00 - val_loss: 5.6210 - val_accuracy: 0.0000e+00\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 282ms/step - loss: 7.8035 - accuracy: 0.0000e+00 - val_loss: 5.7068 - val_accuracy: 0.0000e+00\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 287ms/step - loss: 7.7494 - accuracy: 0.0000e+00 - val_loss: 5.3386 - val_accuracy: 0.0000e+00\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 277ms/step - loss: 7.5664 - accuracy: 0.0000e+00 - val_loss: 4.8746 - val_accuracy: 0.0000e+00\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 278ms/step - loss: 7.0849 - accuracy: 0.0000e+00 - val_loss: 4.4637 - val_accuracy: 0.0000e+00\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 278ms/step - loss: 5.7052 - accuracy: 0.0000e+00 - val_loss: 4.3403 - val_accuracy: 0.0000e+00\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s 278ms/step - loss: 6.6198 - accuracy: 0.0000e+00 - val_loss: 4.0943 - val_accuracy: 0.0000e+00\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - 0s 279ms/step - loss: 6.6584 - accuracy: 0.0000e+00 - val_loss: 3.8893 - val_accuracy: 0.0000e+00\n",
      "Epoch 18/50\n",
      "1/1 [==============================] - 0s 278ms/step - loss: 6.7736 - accuracy: 0.0000e+00 - val_loss: 3.6512 - val_accuracy: 0.0000e+00\n",
      "Epoch 19/50\n",
      "1/1 [==============================] - 0s 279ms/step - loss: 6.2424 - accuracy: 0.0000e+00 - val_loss: 3.5673 - val_accuracy: 0.0000e+00\n",
      "Epoch 20/50\n",
      "1/1 [==============================] - 0s 279ms/step - loss: 6.3472 - accuracy: 0.0000e+00 - val_loss: 3.5075 - val_accuracy: 0.0000e+00\n",
      "Epoch 21/50\n",
      "1/1 [==============================] - 0s 280ms/step - loss: 6.0506 - accuracy: 0.0000e+00 - val_loss: 3.4561 - val_accuracy: 0.0000e+00\n",
      "Epoch 22/50\n",
      "1/1 [==============================] - 0s 282ms/step - loss: 6.4316 - accuracy: 0.0000e+00 - val_loss: 3.4121 - val_accuracy: 0.0000e+00\n",
      "Epoch 23/50\n",
      "1/1 [==============================] - 0s 280ms/step - loss: 5.9702 - accuracy: 0.0000e+00 - val_loss: 3.3691 - val_accuracy: 0.0000e+00\n",
      "Epoch 24/50\n",
      "1/1 [==============================] - 0s 283ms/step - loss: 5.5459 - accuracy: 0.0000e+00 - val_loss: 3.1543 - val_accuracy: 0.0000e+00\n",
      "Epoch 25/50\n",
      "1/1 [==============================] - 0s 278ms/step - loss: 6.5178 - accuracy: 0.0000e+00 - val_loss: 3.1053 - val_accuracy: 0.0000e+00\n",
      "Epoch 26/50\n",
      "1/1 [==============================] - 0s 283ms/step - loss: 6.3050 - accuracy: 0.0000e+00 - val_loss: 2.9098 - val_accuracy: 0.0000e+00\n",
      "Epoch 27/50\n",
      "1/1 [==============================] - 0s 279ms/step - loss: 5.2216 - accuracy: 0.0000e+00 - val_loss: 2.8380 - val_accuracy: 0.0000e+00\n",
      "Epoch 28/50\n",
      "1/1 [==============================] - 0s 277ms/step - loss: 5.5335 - accuracy: 0.0000e+00 - val_loss: 2.7936 - val_accuracy: 0.0000e+00\n",
      "Epoch 29/50\n",
      "1/1 [==============================] - 0s 277ms/step - loss: 5.2759 - accuracy: 0.0000e+00 - val_loss: 2.7551 - val_accuracy: 0.0000e+00\n",
      "Epoch 30/50\n",
      "1/1 [==============================] - 0s 283ms/step - loss: 6.1764 - accuracy: 0.0000e+00 - val_loss: 2.7200 - val_accuracy: 0.0000e+00\n",
      "Epoch 31/50\n",
      "1/1 [==============================] - 0s 280ms/step - loss: 5.1128 - accuracy: 0.0000e+00 - val_loss: 2.6877 - val_accuracy: 0.0000e+00\n",
      "Epoch 32/50\n",
      "1/1 [==============================] - 0s 278ms/step - loss: 4.6891 - accuracy: 0.0000e+00 - val_loss: 2.6575 - val_accuracy: 0.0000e+00\n",
      "Epoch 33/50\n",
      "1/1 [==============================] - 0s 285ms/step - loss: 4.8594 - accuracy: 0.0000e+00 - val_loss: 2.6294 - val_accuracy: 0.0000e+00\n",
      "Epoch 34/50\n",
      "1/1 [==============================] - 0s 287ms/step - loss: 5.5889 - accuracy: 0.0000e+00 - val_loss: 2.6034 - val_accuracy: 0.0000e+00\n",
      "Epoch 35/50\n",
      "1/1 [==============================] - 0s 290ms/step - loss: 5.3639 - accuracy: 0.0000e+00 - val_loss: 2.5851 - val_accuracy: 0.0000e+00\n",
      "Epoch 36/50\n",
      "1/1 [==============================] - 0s 283ms/step - loss: 4.7316 - accuracy: 0.0000e+00 - val_loss: 2.5695 - val_accuracy: 0.0000e+00\n",
      "Epoch 37/50\n",
      "1/1 [==============================] - 0s 283ms/step - loss: 5.1280 - accuracy: 0.0000e+00 - val_loss: 2.3539 - val_accuracy: 0.0000e+00\n",
      "Epoch 38/50\n",
      "1/1 [==============================] - 0s 304ms/step - loss: 5.2527 - accuracy: 0.0000e+00 - val_loss: 2.3314 - val_accuracy: 0.0000e+00\n",
      "Epoch 39/50\n",
      "1/1 [==============================] - 0s 285ms/step - loss: 4.2225 - accuracy: 0.0000e+00 - val_loss: 2.3144 - val_accuracy: 0.0000e+00\n",
      "Epoch 40/50\n",
      "1/1 [==============================] - 0s 311ms/step - loss: 5.1480 - accuracy: 0.0000e+00 - val_loss: 2.2992 - val_accuracy: 0.0000e+00\n",
      "Epoch 41/50\n",
      "1/1 [==============================] - 0s 296ms/step - loss: 4.7437 - accuracy: 0.0000e+00 - val_loss: 2.2860 - val_accuracy: 0.0000e+00\n",
      "Epoch 42/50\n",
      "1/1 [==============================] - 0s 290ms/step - loss: 5.9372 - accuracy: 0.0000e+00 - val_loss: 2.2744 - val_accuracy: 0.0000e+00\n",
      "Epoch 43/50\n",
      "1/1 [==============================] - 0s 282ms/step - loss: 4.5390 - accuracy: 0.0000e+00 - val_loss: 2.2633 - val_accuracy: 0.0000e+00\n",
      "Epoch 44/50\n",
      "1/1 [==============================] - 0s 287ms/step - loss: 5.4384 - accuracy: 0.0000e+00 - val_loss: 2.2525 - val_accuracy: 0.0000e+00\n",
      "Epoch 45/50\n",
      "1/1 [==============================] - 0s 283ms/step - loss: 4.6492 - accuracy: 0.0000e+00 - val_loss: 2.2418 - val_accuracy: 0.0000e+00\n",
      "Epoch 46/50\n",
      "1/1 [==============================] - 0s 290ms/step - loss: 5.2101 - accuracy: 0.0000e+00 - val_loss: 2.2318 - val_accuracy: 0.0000e+00\n",
      "Epoch 47/50\n",
      "1/1 [==============================] - 0s 281ms/step - loss: 4.6789 - accuracy: 0.0000e+00 - val_loss: 2.2221 - val_accuracy: 0.0000e+00\n",
      "Epoch 48/50\n",
      "1/1 [==============================] - 0s 284ms/step - loss: 4.7559 - accuracy: 0.0000e+00 - val_loss: 2.2125 - val_accuracy: 0.0000e+00\n",
      "Epoch 49/50\n",
      "1/1 [==============================] - 0s 282ms/step - loss: 5.1108 - accuracy: 0.0000e+00 - val_loss: 2.2027 - val_accuracy: 0.0000e+00\n",
      "Epoch 50/50\n",
      "1/1 [==============================] - 0s 280ms/step - loss: 4.9847 - accuracy: 0.0000e+00 - val_loss: 2.1924 - val_accuracy: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2d4b28d50>"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_features = X_train_scaled.shape[1]\n",
    "\n",
    "# Reshape the data to include the time steps dimension\n",
    "# -1 tells the program to keep the shape of our origional data set\n",
    "# Let's change our time step from 1 to more; each step is 15 minutes given our data\n",
    "#X_train_reshaped = X_train_scaled.reshape(-1, 100, num_features)\n",
    "#X_test_reshaped = X_test_scaled.reshape(-1, 100, num_features)\n",
    "\n",
    "# We have 2 possible outcomes, and we are trying to predict the stock/indicators to be in position -1 or 1\n",
    "num_classes = 1\n",
    "\n",
    "nn_model = Sequential()\n",
    "\n",
    "# Add LSTM layer\n",
    "nn_model.add(LSTM(units=50, return_sequences=True, input_shape=(X_train_reshaped.shape[1], X_train_reshaped.shape[2])))\n",
    "nn_model.add(Dropout(0.2))\n",
    "\n",
    "# Add another LSTM layer\n",
    "nn_model.add(LSTM(units=50, return_sequences=False))\n",
    "nn_model.add(Dropout(0.2))\n",
    "\n",
    "# Add dense layers\n",
    "nn_model.add(Dense(units=64, activation='relu'))\n",
    "nn_model.add(Dropout(0.2))\n",
    "\n",
    "# Compile model\n",
    "nn_model.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer=\"adam\",\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Summarize model\n",
    "nn_model.summary()\n",
    "\n",
    "\n",
    "# Fit model\n",
    "num_epochs = 50\n",
    "nn_model.fit(X_train_reshaped, y_train, epochs=num_epochs, batch_size=32, validation_split=0.2, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Well, at least the loss is positive... However, now our accuracy is non-existent! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
